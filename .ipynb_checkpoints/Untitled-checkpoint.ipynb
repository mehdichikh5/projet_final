{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc13a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aeb565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.37951</td>\n",
       "      <td>0.39641</td>\n",
       "      <td>2.0472</td>\n",
       "      <td>32.3510</td>\n",
       "      <td>0.38825</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.33050</td>\n",
       "      <td>1.1389</td>\n",
       "      <td>0.50494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.39718</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.658</td>\n",
       "      <td>4.4158</td>\n",
       "      <td>7.4277</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.47225</td>\n",
       "      <td>1.9447</td>\n",
       "      <td>14.7860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.99601</td>\n",
       "      <td>1.6996</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.42002</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.350</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>60.9870</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.69592</td>\n",
       "      <td>0.26713</td>\n",
       "      <td>1.5548</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.43695</td>\n",
       "      <td>1.3090</td>\n",
       "      <td>0.30408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.81774</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.694840</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.270</td>\n",
       "      <td>2.7185</td>\n",
       "      <td>5.2078</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.45879</td>\n",
       "      <td>2.4928</td>\n",
       "      <td>51.9520</td>\n",
       "      <td>0.14988</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.86610</td>\n",
       "      <td>1.0571</td>\n",
       "      <td>0.57353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.14207</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.435</td>\n",
       "      <td>4.2228</td>\n",
       "      <td>5.5497</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.61323</td>\n",
       "      <td>0.22960</td>\n",
       "      <td>1.4063</td>\n",
       "      <td>-7.3128</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.63070</td>\n",
       "      <td>1.1559</td>\n",
       "      <td>0.38677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.48431</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.210</td>\n",
       "      <td>2.8692</td>\n",
       "      <td>7.8980</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Attr1    Attr2    Attr3   Attr4    Attr5    Attr6     Attr7    Attr8  \\\n",
       "0  0.200550  0.37951  0.39641  2.0472  32.3510  0.38825  0.249760  1.33050   \n",
       "1  0.209120  0.49988  0.47225  1.9447  14.7860  0.00000  0.258340  0.99601   \n",
       "2  0.248660  0.69592  0.26713  1.5548  -1.1523  0.00000  0.309060  0.43695   \n",
       "3  0.081483  0.30734  0.45879  2.4928  51.9520  0.14988  0.092704  1.86610   \n",
       "4  0.187320  0.61323  0.22960  1.4063  -7.3128  0.18732  0.187320  0.63070   \n",
       "\n",
       "    Attr9   Attr10  ...    Attr56   Attr57   Attr58    Attr59  Attr60  Attr61  \\\n",
       "0  1.1389  0.50494  ...  0.121960  0.39718  0.87804  0.001924  8.4160  5.1372   \n",
       "1  1.6996  0.49788  ...  0.121300  0.42002  0.85300  0.000000  4.1486  3.2732   \n",
       "2  1.3090  0.30408  ...  0.241140  0.81774  0.76599  0.694840  4.9909  3.9510   \n",
       "3  1.0571  0.57353  ...  0.054015  0.14207  0.94598  0.000000  4.5746  3.6147   \n",
       "4  1.1559  0.38677  ...  0.134850  0.48431  0.86515  0.124440  6.3985  4.3158   \n",
       "\n",
       "    Attr62  Attr63   Attr64  class  \n",
       "0   82.658  4.4158   7.4277   b'0'  \n",
       "1  107.350  3.4000  60.9870   b'0'  \n",
       "2  134.270  2.7185   5.2078   b'0'  \n",
       "3   86.435  4.2228   5.5497   b'0'  \n",
       "4  127.210  2.8692   7.8980   b'0'  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = arff.loadarff('1year.arff')\n",
    "df1 = pd.DataFrame(data1[0])\n",
    "\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983228b",
   "metadata": {},
   "source": [
    "# We see the repartion of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315f0373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class_bar=sns.countplot(data=df1,x=\"class\")\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+500))\n",
    "class_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269a0101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data2 = arff.loadarff('2year.arff')\n",
    "df2 = pd.DataFrame(data2[0])\n",
    "\n",
    "\n",
    "\n",
    "class_bar=sns.countplot(data=df2,x=\"class\")\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+500))\n",
    "class_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4fc8ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEOCAYAAACjJpHCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVgklEQVR4nO3df7BfdZ3f8efLBCK4GySS0JCEDbBxNYBCc4fi2tEOWcbUKkFHnGgpKYVNpay7dtptw852tVszpmJtwREYRpEgrjSClaxI10x2XVyKsJclGoFC0hDJXa4kiLtER9GEd//4nuDXy01y4eR+v/dyn4+Z73zPeZ/zOfdzMmfymvPr801VIUnSS/WKfndAkjS5GSSSpFYMEklSKwaJJKkVg0SS1IpBIklqxSCZ4pLckGRXku921WYl2Zhka/N9bNeyK5JsS/JIkrd11Zck2dIsuzpJmvqJSf4iyQNJvpPk7Qfox6jtJU18BoluBJaNqK0GNlXVImBTM0+SxcAK4NSmzTVJpjVtrgVWAYuaz/5t/iGwvqrObNpec4B+HKi9pAnOIJniquou4OkR5eXAumZ6HXB+V/2Wqnq2qh4DtgFnJZkLzKyqe6rzhutNXW0KmNlMHwM8MbIPh2gvaYKb3u8OaEI6vqqGAapqOMmcpj4P+FbXekNN7efN9Mg6wEeAryf5IPAq4LdG+XvzDtJe0gTnGYlejNHuW9RB6gDvA26sqvnA24HPJxl53B2svaQJziDRaJ5sLjftv+y0q6kPAQu61ptP51LVUDM9sg5wCbAeoKruAV4JHDfi7x2svaQJziDRaDYAK5vplcDtXfUVSWYkOYnOTfH7mstge5Kc3TxtdVFXm8eBpQBJXk8nSHZ3/7FDtJc0wWWqjf573HHH1cKFC/vdjQlj+/bt7Nmzh71793LEEUdwwgkn8OpXv5rt27fzs5/9jCOPPJKTTz6Z6dM7t9OGh4d56qmnSMKCBQs45phjAPjxj3/Mjh07eO655zjmmGNYsGABSfjJT37C9773PZ577jkA5s+fz8yZnXvvDz30EIsXLz5oe0kTw/333/9UVc0ebdmUu9m+cOFCBgcH+90NSZpUknzvQMu8tCVJasUgkSS1YpBIkloxSCRJrRgkkqRWDBJJUivjFiQ9GJ58RpL/2dTvTbJwvPZFknRg43lGciPjOzz5JcAPq+rXgf8O/Ndx2xNJ0gGNW5D0YHjy7m3dCiz1x5Akqfd6/Wb74RyefB6ws9nW3iR/D7wGeGrkH02yis5ZDSeeeGLrnVjy+ze13oZefu6/8qJ+d0Hqi4lys/2lDE8+5qHHq+r6qhqoqoHZs0cdKkaS9BL1OkgO5/Dkz7dJMp3Or++NvJQmSRpnvQ6Swzk8efe23gP8eU21oYwlaQIYt3skSb4I/BPguCRDwIeBtcD6JJfQ+Z2KCwCq6sEk64GHgL3A5VW1r9nUZXSeADsKuLP5AHyWzq/tbaNzJrJivPZFknRg4xYkVfW+AyxaeoD11wBrRqkPAqeNUv8pTRBJkvpnotxslyRNUgaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFYMEklSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFYMEklSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFYMEklSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFb6EiRJ/m2SB5N8N8kXk7wyyawkG5Nsbb6P7Vr/iiTbkjyS5G1d9SVJtjTLrk6SfuyPJE1lPQ+SJPOA3wUGquo0YBqwAlgNbKqqRcCmZp4ki5vlpwLLgGuSTGs2dy2wCljUfJb1cFckSfTv0tZ04Kgk04GjgSeA5cC6Zvk64PxmejlwS1U9W1WPAduAs5LMBWZW1T1VVcBNXW0kST3S8yCpqr8FPgE8DgwDf19VXweOr6rhZp1hYE7TZB6ws2sTQ01tXjM9sv4CSVYlGUwyuHv37sO5O5I05fXj0taxdM4yTgJOAF6V5MKDNRmlVgepv7BYdX1VDVTVwOzZs19slyVJB9GPS1u/BTxWVbur6ufAl4HfBJ5sLlfRfO9q1h8CFnS1n0/nUthQMz2yLknqoX4EyePA2UmObp6yWgo8DGwAVjbrrARub6Y3ACuSzEhyEp2b6vc1l7/2JDm72c5FXW0kST0yvdd/sKruTXIr8DfAXuAB4HrgV4D1SS6hEzYXNOs/mGQ98FCz/uVVta/Z3GXAjcBRwJ3NR5LUQz0PEoCq+jDw4RHlZ+mcnYy2/hpgzSj1QeC0w95BSdKY+Wa7JKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFYMEklSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFYMEklSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFYMEklSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFYMEklSK30JkiSvTnJrkv+b5OEkb0oyK8nGJFub72O71r8iybYkjyR5W1d9SZItzbKrk6Qf+yNJU1m/zkiuAv53Vb0OeCPwMLAa2FRVi4BNzTxJFgMrgFOBZcA1SaY127kWWAUsaj7LerkTkqQ+BEmSmcBbgM8CVNXPqurvgOXAuma1dcD5zfRy4JaqeraqHgO2AWclmQvMrKp7qqqAm7raSJJ6pB9nJCcDu4HPJXkgyWeSvAo4vqqGAZrvOc3684CdXe2Hmtq8ZnpkXZLUQ/0IkunAPwSuraozgR/TXMY6gNHue9RB6i/cQLIqyWCSwd27d7/Y/kqSDqIfQTIEDFXVvc38rXSC5cnmchXN966u9Rd0tZ8PPNHU549Sf4Gqur6qBqpqYPbs2YdtRyRJfQiSqvo+sDPJbzSlpcBDwAZgZVNbCdzeTG8AViSZkeQkOjfV72suf+1JcnbztNZFXW0kST0yvU9/94PAF5IcCWwHLqYTauuTXAI8DlwAUFUPJllPJ2z2ApdX1b5mO5cBNwJHAXc2H0lSD40pSJJsqqqlh6qNVVVtBgZGWTTq9qpqDbBmlPogcNpL6YMk6fA4aJAkeSVwNHBc84Lg/hvcM4ETxrlvkqRJ4FBnJP8a+BCd0LifXwTJM8Cnx69bkqTJ4qBBUlVXAVcl+WBVfapHfZIkTSJjukdSVZ9K8pvAwu42VXXTOPVLkjRJjPVm++eBU4DNwP4npvYPSyJJmsLG+vjvALC4GdNKkqTnjfWFxO8C/2A8OyJJmpzGekZyHPBQkvuAZ/cXq+q8cemVJGnSGGuQfGQ8OyFJmrzG+tTWX453RyRJk9NYn9rawy+GaD8SOAL4cVXNHK+OSZImh7Gekfxq93yS84GzxqNDkqTJ5SUNI19VXwHOObxdkSRNRmO9tPXurtlX0HmvxHdKJEljfmrrnV3Te4EdwPLD3htJ0qQz1nskF493RyRJk9OY7pEkmZ/kfyXZleTJJLclmX/olpKkl7ux3mz/HJ3fTj8BmAf8aVOTJE1xYw2S2VX1uara23xuBGaPY78kSZPEWIPkqSQXJpnWfC4EfjCeHZMkTQ5jDZJ/BbwX+D4wDLwH8Aa8JGnMj//+F2BlVf0QIMks4BN0AkaSNIWN9YzkDftDBKCqngbOHJ8uSZImk7EGySuSHLt/pjkjGevZjCTpZWysYfDfgP+T5FY6Q6O8F1gzbr2SJE0aY32z/aYkg3QGagzw7qp6aFx7JkmaFMZ8eaoJDsNDkvRLXtIw8pIk7WeQSJJaMUgkSa0YJJKkVgwSSVIrBokkqZW+BUkzivADSb7azM9KsjHJ1ua7+036K5JsS/JIkrd11Zck2dIsuzpJ+rEvkjSV9fOM5PeAh7vmVwObqmoRsKmZJ8liYAVwKrAMuCbJtKbNtcAqYFHzWdabrkuS9utLkDQ/0/vPgM90lZcD65rpdcD5XfVbqurZqnoM2AaclWQuMLOq7qmqAm7qaiNJ6pF+nZH8D+A/AM911Y6vqmGA5ntOU58H7Oxab6ipzWumR9ZfIMmqJINJBnfv3n1YdkCS1NHzIEnyDmBXVd0/1iaj1Oog9RcWq66vqoGqGpg9218IlqTDqR9Dwb8ZOC/J24FXAjOT3Aw8mWRuVQ03l612NesPAQu62s8Hnmjq80epS5J6qOdnJFV1RVXNr6qFdG6i/3lVXQhsAFY2q60Ebm+mNwArksxIchKdm+r3NZe/9iQ5u3la66KuNpKkHplIP061Flif5BLgceACgKp6MMl6OiMP7wUur6p9TZvLgBuBo4A7m48kqYf6GiRV9Q3gG830D4ClB1hvDaP8kFZVDQKnjV8PJUmH4pvtkqRWDBJJUisGiSSpFYNEktSKQSJJasUgkSS1YpBIkloxSCRJrRgkkqRWDBJJUisGiSSpFYNEktSKQSJJasUgkSS1YpBIkloxSCRJrRgkkqRWDBJJUisGiSSpFYNEktSKQSJJasUgkSS1YpBIkloxSCRJrRgkkqRWDBJJUisGiSSpFYNEktSKQSJJasUgkSS1YpBIkloxSCRJrfQ8SJIsSPIXSR5O8mCS32vqs5JsTLK1+T62q80VSbYleSTJ27rqS5JsaZZdnSS93h9Jmur6cUayF/h3VfV64Gzg8iSLgdXApqpaBGxq5mmWrQBOBZYB1ySZ1mzrWmAVsKj5LOvljkiS+hAkVTVcVX/TTO8BHgbmAcuBdc1q64Dzm+nlwC1V9WxVPQZsA85KMheYWVX3VFUBN3W1kST1SF/vkSRZCJwJ3AscX1XD0AkbYE6z2jxgZ1ezoaY2r5keWR/t76xKMphkcPfu3Yd1HyRpqutbkCT5FeA24ENV9czBVh2lVgepv7BYdX1VDVTVwOzZs198ZyVJB9SXIElyBJ0Q+UJVfbkpP9lcrqL53tXUh4AFXc3nA0809fmj1CVJPdSPp7YCfBZ4uKo+2bVoA7CymV4J3N5VX5FkRpKT6NxUv6+5/LUnydnNNi/qaiNJ6pHpffibbwb+BbAlyeam9gfAWmB9kkuAx4ELAKrqwSTrgYfoPPF1eVXta9pdBtwIHAXc2XwkST3U8yCpqr9i9PsbAEsP0GYNsGaU+iBw2uHrnSTpxfLNdklSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmtGCSSpFYMEkkT2r59+zjzzDN5xzveAcC3v/1t3vSmN3H66afzzne+k2ee6Yz5umPHDo466ijOOOMMzjjjDD7wgQ+Mur2nn36ac889l0WLFnHuuefywx/+sGf78nJlkEia0K666ipe//rXPz9/6aWXsnbtWrZs2cK73vUurrzyyueXnXLKKWzevJnNmzdz3XXXjbq9tWvXsnTpUrZu3crSpUtZu3btuO/Dy51BImnCGhoa4o477uDSSy99vvbII4/wlre8BYBzzz2X22677UVt8/bbb2flys74sCtXruQrX/nKYevvVGWQSJqwPvShD/Hxj3+cV7ziF/9VnXbaaWzYsAGAL33pS+zc+YvfvXvsscc488wzeetb38o3v/nNUbf55JNPMnfuXADmzp3Lrl27Rl1PY2eQSJqQvvrVrzJnzhyWLFnyS/UbbriBT3/60yxZsoQ9e/Zw5JFHAp1QePzxx3nggQf45Cc/yfvf//7n759ofPVjGHlJOqS7776bDRs28LWvfY2f/vSnPPPMM1x44YXcfPPNfP3rXwfg0Ucf5Y477gBgxowZzJgxA4AlS5Zwyimn8OijjzIwMPBL2z3++OMZHh5m7ty5DA8PM2fOHNSOZySSJqSPfexjDA0NsWPHDm655RbOOeccbr755ucvRT333HN89KMfff7prN27d7NvX+enirZv387WrVs5+eSTX7Dd8847j3Xr1gGwbt06li9f3qM9evkySCRNKl/84hd57Wtfy+te9zpOOOEELr74YgDuuusu3vCGN/DGN76R97znPVx33XXMmjUL6DzpNTg4CMDq1avZuHEjixYtYuPGjaxevbpv+/Jykarqdx96amBgoPYfUC/Vkt+/6TD1Ri8n9195Ub+7II2bJPdX1cBoyzwjkSS1YpBIkloxSCRJrRgkkqRWDBJJUisGiSSpFYNEktSKQSJJasUgkSS14qCN0svI4398er+7oAnoxD/aMq7b94xEktSKQSJJasUgkSS1YpBIkloxSCRJrRgkkqRWJn2QJFmW5JEk25L4U2eS1GOTOkiSTAM+DfxTYDHwviSL+9srSZpaJnWQAGcB26pqe1X9DLgFWN7nPknSlDLZ32yfB+zsmh8C/tHIlZKsAlY1sz9K8kgP+jZVHAc81e9OTAT5xMp+d0G/zGNzvw/ncGzl1w60YLIHyWj/OvWCQtX1wPXj352pJ8lgVQ30ux/SSB6bvTPZL20NAQu65ucDT/SpL5I0JU32IPlrYFGSk5IcCawANvS5T5I0pUzqS1tVtTfJ7wB/BkwDbqiqB/vcranGS4aaqDw2eyRVL7ilIEnSmE32S1uSpD4zSCRJrRgkOqAkC5N8d5T6N5IsbKaXJNnSDFFzdZI09Y8k+Ze97bGmkjEen2uS7EzyoxHreHweRgaJ2rqWzsuei5rPsv52R/olf0pnBAyNI4NEhzI9ybok30lya5KjgaeBfUnmAjOr6p7qPLVxE3B+0+5HwE/602VNIQc8PgGq6ltVNTxKO4/Pw2hSP/6rnvgN4JKqujvJDcC/qap3AyQZoPNS6H5DdIatoao+0fOeaio64PF5MB6fh5dnJDqUnVV1dzN9M/CPu5aNaYgaaRwd7PhUjxgkOpSRwdA9P0RnWJr9HKJGvXaw41M9YpDoUE5M8qZm+n3AX+1f0Fx73pPk7OZprYuA2/vQR01dBzw+1TsGiQ7lYWBlku8As+g8pdXtMuAzwDbg/wF39rZ7muIOenwm+XiSIeDoJENJPtKHPr7sOUSKJKkVz0gkSa0YJJKkVgwSSVIrBokkqRWDRJLUikEi9VAz6uy/73c/pMPJIJEktWKQSOMoyUXNyLTfTvL5Ect+O8lfN8tua0auJckFSb7b1O9qaqcmuS/J5mZ7i/qxP9JofCFRGidJTgW+DLy5qp5KMgv4XeBHVfWJJK+pqh80634UeLKqPpVkC7Csqv42yaur6u+SfAr4VlV9IcmRwLSqchh0TQiekUjj5xzg1qp6CqCqnh6x/LQk32yC458Dpzb1u4Ebk/w2MK2p3QP8QZL/CPyaIaKJxCCRxk84+Gi0NwK/U1WnA/8ZeCVAVX0A+ENgAbC5OXP5E+A8Oj/G9GdJzhnPjksvhkEijZ9NwHuTvAagubTV7VeB4SRH0DkjoVnvlKq6t6r+CHgKWJDkZGB7VV0NbADe0JM9kMbAX0iUxklVPZhkDfCXSfYBDwA7ulb5T8C9wPeALXSCBeDK5mZ66ITRt4HVwIVJfg58H/jjnuyENAbebJckteKlLUlSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWjFIJEmt/H814SsLVYjeyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data3 = arff.loadarff('3year.arff')\n",
    "df3 = pd.DataFrame(data3[0])\n",
    "\n",
    "\n",
    "class_bar=sns.countplot(data=df3,x=\"class\")\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+500))\n",
    "class_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589286d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data4 = arff.loadarff('4year.arff')\n",
    "df4 = pd.DataFrame(data4[0])\n",
    "\n",
    "\n",
    "class_bar=sns.countplot(data=df4,x=\"class\")\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+500))\n",
    "class_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b25672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data5 = arff.loadarff('5year.arff')\n",
    "df5 = pd.DataFrame(data5[0])\n",
    "\n",
    "\n",
    "\n",
    "class_bar=sns.countplot(data=df5,x=\"class\")\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+500))\n",
    "class_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab48da",
   "metadata": {},
   "source": [
    "## We can observe that all the dataset are imbalanced, there's a lot more of b'0 class. So if we train our data with this imbalance dataset and test it later with the new testing data, our model will be a lot partial towards the class 'b0'. So we came with this solution: We'll take all the b1 class from the five dataset et take 500 b1 class  randomly from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186e5b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6756</th>\n",
       "      <td>0.030372</td>\n",
       "      <td>0.874460</td>\n",
       "      <td>0.081671</td>\n",
       "      <td>1.09500</td>\n",
       "      <td>-76.581</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.038522</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>1.96770</td>\n",
       "      <td>0.125540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.241920</td>\n",
       "      <td>0.97971</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>4.0379</td>\n",
       "      <td>4.6154</td>\n",
       "      <td>159.520</td>\n",
       "      <td>2.2881</td>\n",
       "      <td>33.72300</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>0.032686</td>\n",
       "      <td>0.827750</td>\n",
       "      <td>0.020096</td>\n",
       "      <td>1.05040</td>\n",
       "      <td>-41.614</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.041231</td>\n",
       "      <td>0.208090</td>\n",
       "      <td>1.05460</td>\n",
       "      <td>0.172250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041392</td>\n",
       "      <td>0.189770</td>\n",
       "      <td>0.96152</td>\n",
       "      <td>1.415700</td>\n",
       "      <td>9.4101</td>\n",
       "      <td>3.9256</td>\n",
       "      <td>137.870</td>\n",
       "      <td>2.6474</td>\n",
       "      <td>1.81340</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>0.649890</td>\n",
       "      <td>0.098723</td>\n",
       "      <td>0.483760</td>\n",
       "      <td>5.90020</td>\n",
       "      <td>264.600</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.815270</td>\n",
       "      <td>9.128800</td>\n",
       "      <td>1.46040</td>\n",
       "      <td>0.901230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557580</td>\n",
       "      <td>0.721120</td>\n",
       "      <td>0.44213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.1698</td>\n",
       "      <td>24.673</td>\n",
       "      <td>14.7930</td>\n",
       "      <td>3.49840</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6759</th>\n",
       "      <td>0.075803</td>\n",
       "      <td>0.760570</td>\n",
       "      <td>-0.199050</td>\n",
       "      <td>0.51163</td>\n",
       "      <td>-57.769</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.089522</td>\n",
       "      <td>0.314760</td>\n",
       "      <td>1.65730</td>\n",
       "      <td>0.239400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239020</td>\n",
       "      <td>0.316640</td>\n",
       "      <td>0.78417</td>\n",
       "      <td>1.196100</td>\n",
       "      <td>35.6190</td>\n",
       "      <td>10.8850</td>\n",
       "      <td>89.765</td>\n",
       "      <td>4.0662</td>\n",
       "      <td>2.09410</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6760</th>\n",
       "      <td>-0.179970</td>\n",
       "      <td>0.953480</td>\n",
       "      <td>-0.346650</td>\n",
       "      <td>0.63643</td>\n",
       "      <td>-207.040</td>\n",
       "      <td>-0.43095</td>\n",
       "      <td>-0.179970</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>1.19990</td>\n",
       "      <td>0.046519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026425</td>\n",
       "      <td>-3.868800</td>\n",
       "      <td>0.96751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.9261</td>\n",
       "      <td>6.1379</td>\n",
       "      <td>290.030</td>\n",
       "      <td>1.2585</td>\n",
       "      <td>3.05190</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>0.012898</td>\n",
       "      <td>0.706210</td>\n",
       "      <td>0.038857</td>\n",
       "      <td>1.17220</td>\n",
       "      <td>-18.907</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>1.67680</td>\n",
       "      <td>0.293790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020169</td>\n",
       "      <td>0.043904</td>\n",
       "      <td>1.01220</td>\n",
       "      <td>1.259400</td>\n",
       "      <td>13.4720</td>\n",
       "      <td>12.4320</td>\n",
       "      <td>49.117</td>\n",
       "      <td>7.4313</td>\n",
       "      <td>2.27990</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>-0.578050</td>\n",
       "      <td>0.967020</td>\n",
       "      <td>-0.800850</td>\n",
       "      <td>0.16576</td>\n",
       "      <td>-67.365</td>\n",
       "      <td>-0.57805</td>\n",
       "      <td>-0.578050</td>\n",
       "      <td>-0.403340</td>\n",
       "      <td>0.93979</td>\n",
       "      <td>-0.390040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064073</td>\n",
       "      <td>1.482000</td>\n",
       "      <td>1.06410</td>\n",
       "      <td>-0.018084</td>\n",
       "      <td>110.7200</td>\n",
       "      <td>44.7590</td>\n",
       "      <td>81.220</td>\n",
       "      <td>4.4940</td>\n",
       "      <td>5.13050</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>-0.179050</td>\n",
       "      <td>1.255300</td>\n",
       "      <td>-0.275990</td>\n",
       "      <td>0.74554</td>\n",
       "      <td>-120.440</td>\n",
       "      <td>-0.17905</td>\n",
       "      <td>-0.154930</td>\n",
       "      <td>-0.260180</td>\n",
       "      <td>1.17490</td>\n",
       "      <td>-0.326590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148880</td>\n",
       "      <td>0.548240</td>\n",
       "      <td>0.85112</td>\n",
       "      <td>-0.522430</td>\n",
       "      <td>9.8526</td>\n",
       "      <td>3.4892</td>\n",
       "      <td>207.870</td>\n",
       "      <td>1.7559</td>\n",
       "      <td>9.95270</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>-0.108860</td>\n",
       "      <td>0.743940</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>1.08780</td>\n",
       "      <td>-17.003</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.109180</td>\n",
       "      <td>0.125310</td>\n",
       "      <td>0.84516</td>\n",
       "      <td>0.093224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183200</td>\n",
       "      <td>-1.167700</td>\n",
       "      <td>1.18320</td>\n",
       "      <td>6.092400</td>\n",
       "      <td>13.8860</td>\n",
       "      <td>6.0769</td>\n",
       "      <td>83.122</td>\n",
       "      <td>4.3911</td>\n",
       "      <td>0.95575</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>-0.105370</td>\n",
       "      <td>0.536290</td>\n",
       "      <td>-0.045578</td>\n",
       "      <td>0.91478</td>\n",
       "      <td>-56.068</td>\n",
       "      <td>-0.10537</td>\n",
       "      <td>-0.109940</td>\n",
       "      <td>0.864600</td>\n",
       "      <td>0.95040</td>\n",
       "      <td>0.463670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.227250</td>\n",
       "      <td>1.05220</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>7.7332</td>\n",
       "      <td>4.7174</td>\n",
       "      <td>136.850</td>\n",
       "      <td>2.6672</td>\n",
       "      <td>2.79270</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2091 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Attr1     Attr2     Attr3    Attr4    Attr5    Attr6     Attr7  \\\n",
       "6756  0.030372  0.874460  0.081671  1.09500  -76.581  0.00000  0.038522   \n",
       "6757  0.032686  0.827750  0.020096  1.05040  -41.614  0.00000  0.041231   \n",
       "6758  0.649890  0.098723  0.483760  5.90020  264.600  0.00000  0.815270   \n",
       "6759  0.075803  0.760570 -0.199050  0.51163  -57.769  0.00000  0.089522   \n",
       "6760 -0.179970  0.953480 -0.346650  0.63643 -207.040 -0.43095 -0.179970   \n",
       "...        ...       ...       ...      ...      ...      ...       ...   \n",
       "5905  0.012898  0.706210  0.038857  1.17220  -18.907  0.00000  0.013981   \n",
       "5906 -0.578050  0.967020 -0.800850  0.16576  -67.365 -0.57805 -0.578050   \n",
       "5907 -0.179050  1.255300 -0.275990  0.74554 -120.440 -0.17905 -0.154930   \n",
       "5908 -0.108860  0.743940  0.015449  1.08780  -17.003 -0.10886 -0.109180   \n",
       "5909 -0.105370  0.536290 -0.045578  0.91478  -56.068 -0.10537 -0.109940   \n",
       "\n",
       "         Attr8    Attr9    Attr10  ...    Attr56    Attr57   Attr58    Attr59  \\\n",
       "6756  0.143570  1.96770  0.125540  ...  0.001575  0.241920  0.97971  0.034768   \n",
       "6757  0.208090  1.05460  0.172250  ...  0.041392  0.189770  0.96152  1.415700   \n",
       "6758  9.128800  1.46040  0.901230  ...  0.557580  0.721120  0.44213  0.000000   \n",
       "6759  0.314760  1.65730  0.239400  ...  0.239020  0.316640  0.78417  1.196100   \n",
       "6760  0.048788  1.19990  0.046519  ...  0.026425 -3.868800  0.96751  0.000000   \n",
       "...        ...      ...       ...  ...       ...       ...      ...       ...   \n",
       "5905  0.416000  1.67680  0.293790  ...  0.020169  0.043904  1.01220  1.259400   \n",
       "5906 -0.403340  0.93979 -0.390040  ... -0.064073  1.482000  1.06410 -0.018084   \n",
       "5907 -0.260180  1.17490 -0.326590  ...  0.148880  0.548240  0.85112 -0.522430   \n",
       "5908  0.125310  0.84516  0.093224  ... -0.183200 -1.167700  1.18320  6.092400   \n",
       "5909  0.864600  0.95040  0.463670  ... -0.052186 -0.227250  1.05220  0.003196   \n",
       "\n",
       "        Attr60   Attr61   Attr62   Attr63    Attr64  class  \n",
       "6756    4.0379   4.6154  159.520   2.2881  33.72300   b'1'  \n",
       "6757    9.4101   3.9256  137.870   2.6474   1.81340   b'1'  \n",
       "6758       NaN   5.1698   24.673  14.7930   3.49840   b'1'  \n",
       "6759   35.6190  10.8850   89.765   4.0662   2.09410   b'1'  \n",
       "6760    2.9261   6.1379  290.030   1.2585   3.05190   b'1'  \n",
       "...        ...      ...      ...      ...       ...    ...  \n",
       "5905   13.4720  12.4320   49.117   7.4313   2.27990   b'1'  \n",
       "5906  110.7200  44.7590   81.220   4.4940   5.13050   b'1'  \n",
       "5907    9.8526   3.4892  207.870   1.7559   9.95270   b'1'  \n",
       "5908   13.8860   6.0769   83.122   4.3911   0.95575   b'1'  \n",
       "5909    7.7332   4.7174  136.850   2.6672   2.79270   b'1'  \n",
       "\n",
       "[2091 rows x 65 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_class = df1['class'].unique()\n",
    "\n",
    "\n",
    "df_class = df1[df1['class'] == values_class[1]]\n",
    "df_class = df_class.append(df2[df2['class'] == values_class[1]])\n",
    "df_class = df_class.append(df3[df3['class'] == values_class[1]])\n",
    "df_class = df_class.append(df4[df4['class'] == values_class[1]])\n",
    "df_class = df_class.append(df5[df5['class'] == values_class[1]])\n",
    "df_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244935b",
   "metadata": {},
   "source": [
    "## There's 2091 rows of class b1, we're gonna add 4000 rows of class b0 to this dataframe. So we're gonna take 800 rows from each dataset (4000/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c77865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6756</th>\n",
       "      <td>0.030372</td>\n",
       "      <td>0.874460</td>\n",
       "      <td>0.081671</td>\n",
       "      <td>1.09500</td>\n",
       "      <td>-76.5810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038522</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>1.96770</td>\n",
       "      <td>0.125540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.241920</td>\n",
       "      <td>0.97971</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>4.0379</td>\n",
       "      <td>4.6154</td>\n",
       "      <td>159.520</td>\n",
       "      <td>2.2881</td>\n",
       "      <td>33.72300</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>0.032686</td>\n",
       "      <td>0.827750</td>\n",
       "      <td>0.020096</td>\n",
       "      <td>1.05040</td>\n",
       "      <td>-41.6140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041231</td>\n",
       "      <td>0.208090</td>\n",
       "      <td>1.05460</td>\n",
       "      <td>0.172250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041392</td>\n",
       "      <td>0.189770</td>\n",
       "      <td>0.96152</td>\n",
       "      <td>1.415700</td>\n",
       "      <td>9.4101</td>\n",
       "      <td>3.9256</td>\n",
       "      <td>137.870</td>\n",
       "      <td>2.6474</td>\n",
       "      <td>1.81340</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>0.649890</td>\n",
       "      <td>0.098723</td>\n",
       "      <td>0.483760</td>\n",
       "      <td>5.90020</td>\n",
       "      <td>264.6000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.815270</td>\n",
       "      <td>9.128800</td>\n",
       "      <td>1.46040</td>\n",
       "      <td>0.901230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557580</td>\n",
       "      <td>0.721120</td>\n",
       "      <td>0.44213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.1698</td>\n",
       "      <td>24.673</td>\n",
       "      <td>14.7930</td>\n",
       "      <td>3.49840</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6759</th>\n",
       "      <td>0.075803</td>\n",
       "      <td>0.760570</td>\n",
       "      <td>-0.199050</td>\n",
       "      <td>0.51163</td>\n",
       "      <td>-57.7690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089522</td>\n",
       "      <td>0.314760</td>\n",
       "      <td>1.65730</td>\n",
       "      <td>0.239400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239020</td>\n",
       "      <td>0.316640</td>\n",
       "      <td>0.78417</td>\n",
       "      <td>1.196100</td>\n",
       "      <td>35.6190</td>\n",
       "      <td>10.8850</td>\n",
       "      <td>89.765</td>\n",
       "      <td>4.0662</td>\n",
       "      <td>2.09410</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6760</th>\n",
       "      <td>-0.179970</td>\n",
       "      <td>0.953480</td>\n",
       "      <td>-0.346650</td>\n",
       "      <td>0.63643</td>\n",
       "      <td>-207.0400</td>\n",
       "      <td>-0.430950</td>\n",
       "      <td>-0.179970</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>1.19990</td>\n",
       "      <td>0.046519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026425</td>\n",
       "      <td>-3.868800</td>\n",
       "      <td>0.96751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.9261</td>\n",
       "      <td>6.1379</td>\n",
       "      <td>290.030</td>\n",
       "      <td>1.2585</td>\n",
       "      <td>3.05190</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.492130</td>\n",
       "      <td>0.340140</td>\n",
       "      <td>1.71650</td>\n",
       "      <td>-17.4570</td>\n",
       "      <td>0.055631</td>\n",
       "      <td>0.036057</td>\n",
       "      <td>0.984920</td>\n",
       "      <td>1.05390</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051168</td>\n",
       "      <td>0.056558</td>\n",
       "      <td>0.94883</td>\n",
       "      <td>0.035896</td>\n",
       "      <td>3.3451</td>\n",
       "      <td>3.1864</td>\n",
       "      <td>132.700</td>\n",
       "      <td>2.7505</td>\n",
       "      <td>7.05320</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>-0.017694</td>\n",
       "      <td>0.156490</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>1.80130</td>\n",
       "      <td>-110.6500</td>\n",
       "      <td>0.727860</td>\n",
       "      <td>-0.019055</td>\n",
       "      <td>5.390300</td>\n",
       "      <td>0.22237</td>\n",
       "      <td>0.843510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.88465</td>\n",
       "      <td>0.092083</td>\n",
       "      <td>3.9446</td>\n",
       "      <td>7.6024</td>\n",
       "      <td>129.370</td>\n",
       "      <td>2.8214</td>\n",
       "      <td>0.25916</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>0.096783</td>\n",
       "      <td>0.615750</td>\n",
       "      <td>0.292050</td>\n",
       "      <td>1.83930</td>\n",
       "      <td>2.7187</td>\n",
       "      <td>0.191210</td>\n",
       "      <td>0.120320</td>\n",
       "      <td>0.583130</td>\n",
       "      <td>1.09310</td>\n",
       "      <td>0.359060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085159</td>\n",
       "      <td>0.269540</td>\n",
       "      <td>0.91484</td>\n",
       "      <td>0.745830</td>\n",
       "      <td>6.3517</td>\n",
       "      <td>7.6355</td>\n",
       "      <td>72.574</td>\n",
       "      <td>5.0294</td>\n",
       "      <td>4.86100</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>0.019835</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.436250</td>\n",
       "      <td>6.39910</td>\n",
       "      <td>133.8100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>11.376000</td>\n",
       "      <td>1.18600</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006544</td>\n",
       "      <td>0.021579</td>\n",
       "      <td>0.97954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8934</td>\n",
       "      <td>24.868</td>\n",
       "      <td>14.6780</td>\n",
       "      <td>2.45570</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4105</th>\n",
       "      <td>0.138560</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>0.712330</td>\n",
       "      <td>5.47140</td>\n",
       "      <td>114.9800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171990</td>\n",
       "      <td>5.277200</td>\n",
       "      <td>1.33260</td>\n",
       "      <td>0.840690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120620</td>\n",
       "      <td>0.164820</td>\n",
       "      <td>0.87650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.8999</td>\n",
       "      <td>6.2966</td>\n",
       "      <td>43.636</td>\n",
       "      <td>8.3647</td>\n",
       "      <td>10.38100</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6091 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Attr1     Attr2     Attr3    Attr4     Attr5     Attr6     Attr7  \\\n",
       "6756  0.030372  0.874460  0.081671  1.09500  -76.5810  0.000000  0.038522   \n",
       "6757  0.032686  0.827750  0.020096  1.05040  -41.6140  0.000000  0.041231   \n",
       "6758  0.649890  0.098723  0.483760  5.90020  264.6000  0.000000  0.815270   \n",
       "6759  0.075803  0.760570 -0.199050  0.51163  -57.7690  0.000000  0.089522   \n",
       "6760 -0.179970  0.953480 -0.346650  0.63643 -207.0400 -0.430950 -0.179970   \n",
       "...        ...       ...       ...      ...       ...       ...       ...   \n",
       "2446  0.027414  0.492130  0.340140  1.71650  -17.4570  0.055631  0.036057   \n",
       "1307 -0.017694  0.156490  0.063158  1.80130 -110.6500  0.727860 -0.019055   \n",
       "3709  0.096783  0.615750  0.292050  1.83930    2.7187  0.191210  0.120320   \n",
       "2568  0.019835  0.080801  0.436250  6.39910  133.8100  0.000000  0.025051   \n",
       "4105  0.138560  0.159310  0.712330  5.47140  114.9800  0.000000  0.171990   \n",
       "\n",
       "          Attr8    Attr9    Attr10  ...    Attr56    Attr57   Attr58  \\\n",
       "6756   0.143570  1.96770  0.125540  ...  0.001575  0.241920  0.97971   \n",
       "6757   0.208090  1.05460  0.172250  ...  0.041392  0.189770  0.96152   \n",
       "6758   9.128800  1.46040  0.901230  ...  0.557580  0.721120  0.44213   \n",
       "6759   0.314760  1.65730  0.239400  ...  0.239020  0.316640  0.78417   \n",
       "6760   0.048788  1.19990  0.046519  ...  0.026425 -3.868800  0.96751   \n",
       "...         ...      ...       ...  ...       ...       ...      ...   \n",
       "2446   0.984920  1.05390  0.484710  ...  0.051168  0.056558  0.94883   \n",
       "1307   5.390300  0.22237  0.843510  ...  0.239320  0.000000  0.88465   \n",
       "3709   0.583130  1.09310  0.359060  ...  0.085159  0.269540  0.91484   \n",
       "2568  11.376000  1.18600  0.919200  ... -0.006544  0.021579  0.97954   \n",
       "4105   5.277200  1.33260  0.840690  ...  0.120620  0.164820  0.87650   \n",
       "\n",
       "        Attr59   Attr60   Attr61   Attr62   Attr63    Attr64  class  \n",
       "6756  0.034768   4.0379   4.6154  159.520   2.2881  33.72300   b'1'  \n",
       "6757  1.415700   9.4101   3.9256  137.870   2.6474   1.81340   b'1'  \n",
       "6758  0.000000      NaN   5.1698   24.673  14.7930   3.49840   b'1'  \n",
       "6759  1.196100  35.6190  10.8850   89.765   4.0662   2.09410   b'1'  \n",
       "6760  0.000000   2.9261   6.1379  290.030   1.2585   3.05190   b'1'  \n",
       "...        ...      ...      ...      ...      ...       ...    ...  \n",
       "2446  0.035896   3.3451   3.1864  132.700   2.7505   7.05320   b'0'  \n",
       "1307  0.092083   3.9446   7.6024  129.370   2.8214   0.25916   b'0'  \n",
       "3709  0.745830   6.3517   7.6355   72.574   5.0294   4.86100   b'0'  \n",
       "2568  0.000000      NaN   3.8934   24.868  14.6780   2.45570   b'0'  \n",
       "4105  0.000000   3.8999   6.2966   43.636   8.3647  10.38100   b'0'  \n",
       "\n",
       "[6091 rows x 65 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_class.append(df1[df1['class'] == values_class[0]].sample(n=800))\n",
    "df = df.append(df2[df2['class'] == values_class[0]].sample(n=800))\n",
    "df = df.append(df3[df3['class'] == values_class[0]].sample(n=800))\n",
    "df = df.append(df4[df4['class'] == values_class[0]].sample(n=800))\n",
    "df = df.append(df5[df5['class'] == values_class[0]].sample(n=800))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908c7b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_bar=sns.countplot(data=df,x=\"class\")\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+500))\n",
    "class_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce3801",
   "metadata": {},
   "source": [
    "# the dataset is better balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de7fc9",
   "metadata": {},
   "source": [
    "# Check of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25836076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6091 entries, 6756 to 4105\n",
      "Data columns (total 65 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Attr1   6090 non-null   float64\n",
      " 1   Attr2   6090 non-null   float64\n",
      " 2   Attr3   6090 non-null   float64\n",
      " 3   Attr4   6081 non-null   float64\n",
      " 4   Attr5   6081 non-null   float64\n",
      " 5   Attr6   6090 non-null   float64\n",
      " 6   Attr7   6090 non-null   float64\n",
      " 7   Attr8   6084 non-null   float64\n",
      " 8   Attr9   6090 non-null   float64\n",
      " 9   Attr10  6090 non-null   float64\n",
      " 10  Attr11  6054 non-null   float64\n",
      " 11  Attr12  6081 non-null   float64\n",
      " 12  Attr13  6071 non-null   float64\n",
      " 13  Attr14  6090 non-null   float64\n",
      " 14  Attr15  6091 non-null   float64\n",
      " 15  Attr16  6084 non-null   float64\n",
      " 16  Attr17  6084 non-null   float64\n",
      " 17  Attr18  6090 non-null   float64\n",
      " 18  Attr19  6071 non-null   float64\n",
      " 19  Attr20  6071 non-null   float64\n",
      " 20  Attr21  4970 non-null   float64\n",
      " 21  Attr22  6090 non-null   float64\n",
      " 22  Attr23  6071 non-null   float64\n",
      " 23  Attr24  5985 non-null   float64\n",
      " 24  Attr25  6090 non-null   float64\n",
      " 25  Attr26  6084 non-null   float64\n",
      " 26  Attr27  5193 non-null   float64\n",
      " 27  Attr28  5933 non-null   float64\n",
      " 28  Attr29  6090 non-null   float64\n",
      " 29  Attr30  6071 non-null   float64\n",
      " 30  Attr31  6071 non-null   float64\n",
      " 31  Attr32  6044 non-null   float64\n",
      " 32  Attr33  6081 non-null   float64\n",
      " 33  Attr34  6084 non-null   float64\n",
      " 34  Attr35  6090 non-null   float64\n",
      " 35  Attr36  6090 non-null   float64\n",
      " 36  Attr37  3356 non-null   float64\n",
      " 37  Attr38  6090 non-null   float64\n",
      " 38  Attr39  6071 non-null   float64\n",
      " 39  Attr40  6081 non-null   float64\n",
      " 40  Attr41  6026 non-null   float64\n",
      " 41  Attr42  6071 non-null   float64\n",
      " 42  Attr43  6071 non-null   float64\n",
      " 43  Attr44  6071 non-null   float64\n",
      " 44  Attr45  5732 non-null   float64\n",
      " 45  Attr46  6080 non-null   float64\n",
      " 46  Attr47  6065 non-null   float64\n",
      " 47  Attr48  6090 non-null   float64\n",
      " 48  Attr49  6071 non-null   float64\n",
      " 49  Attr50  6084 non-null   float64\n",
      " 50  Attr51  6090 non-null   float64\n",
      " 51  Attr52  6064 non-null   float64\n",
      " 52  Attr53  5933 non-null   float64\n",
      " 53  Attr54  5933 non-null   float64\n",
      " 54  Attr55  6091 non-null   float64\n",
      " 55  Attr56  6071 non-null   float64\n",
      " 56  Attr57  6089 non-null   float64\n",
      " 57  Attr58  6077 non-null   float64\n",
      " 58  Attr59  6089 non-null   float64\n",
      " 59  Attr60  5730 non-null   float64\n",
      " 60  Attr61  6075 non-null   float64\n",
      " 61  Attr62  6071 non-null   float64\n",
      " 62  Attr63  6081 non-null   float64\n",
      " 63  Attr64  5933 non-null   float64\n",
      " 64  class   6091 non-null   object \n",
      "dtypes: float64(64), object(1)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddaf13c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6091, 65)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82ade049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr55</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6090.000000</td>\n",
       "      <td>6090.000000</td>\n",
       "      <td>6090.000000</td>\n",
       "      <td>6081.000000</td>\n",
       "      <td>6.081000e+03</td>\n",
       "      <td>6090.000000</td>\n",
       "      <td>6090.000000</td>\n",
       "      <td>6084.000000</td>\n",
       "      <td>6090.000000</td>\n",
       "      <td>6090.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6071.000000</td>\n",
       "      <td>6089.000000</td>\n",
       "      <td>6077.000000</td>\n",
       "      <td>6089.000000</td>\n",
       "      <td>5730.000000</td>\n",
       "      <td>6075.000000</td>\n",
       "      <td>6.071000e+03</td>\n",
       "      <td>6081.000000</td>\n",
       "      <td>5933.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.080979</td>\n",
       "      <td>0.924284</td>\n",
       "      <td>-0.192666</td>\n",
       "      <td>5.525081</td>\n",
       "      <td>-1.594755e+02</td>\n",
       "      <td>-0.472636</td>\n",
       "      <td>-0.067617</td>\n",
       "      <td>19.953860</td>\n",
       "      <td>1.811503</td>\n",
       "      <td>0.122745</td>\n",
       "      <td>...</td>\n",
       "      <td>5082.324856</td>\n",
       "      <td>-1.433530</td>\n",
       "      <td>-0.603328</td>\n",
       "      <td>13.585445</td>\n",
       "      <td>1.979408</td>\n",
       "      <td>187.357968</td>\n",
       "      <td>17.000457</td>\n",
       "      <td>1.837821e+03</td>\n",
       "      <td>10.062505</td>\n",
       "      <td>82.390377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.081010</td>\n",
       "      <td>11.668831</td>\n",
       "      <td>11.635570</td>\n",
       "      <td>93.418738</td>\n",
       "      <td>1.962969e+04</td>\n",
       "      <td>13.186012</td>\n",
       "      <td>6.083108</td>\n",
       "      <td>747.706288</td>\n",
       "      <td>3.136768</td>\n",
       "      <td>12.230470</td>\n",
       "      <td>...</td>\n",
       "      <td>34362.636399</td>\n",
       "      <td>97.328136</td>\n",
       "      <td>28.044677</td>\n",
       "      <td>790.781341</td>\n",
       "      <td>100.666203</td>\n",
       "      <td>4256.876238</td>\n",
       "      <td>275.865970</td>\n",
       "      <td>9.481545e+04</td>\n",
       "      <td>105.798150</td>\n",
       "      <td>2539.423334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-463.890000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-479.960000</td>\n",
       "      <td>-0.403110</td>\n",
       "      <td>-1.076400e+06</td>\n",
       "      <td>-508.410000</td>\n",
       "      <td>-463.890000</td>\n",
       "      <td>-2.003200</td>\n",
       "      <td>-3.496000</td>\n",
       "      <td>-479.910000</td>\n",
       "      <td>...</td>\n",
       "      <td>-311100.000000</td>\n",
       "      <td>-7522.100000</td>\n",
       "      <td>-1667.300000</td>\n",
       "      <td>-0.085920</td>\n",
       "      <td>-256.990000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003150</td>\n",
       "      <td>-1.496500e+04</td>\n",
       "      <td>-1.543200</td>\n",
       "      <td>-0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.023993</td>\n",
       "      <td>0.312793</td>\n",
       "      <td>-0.031263</td>\n",
       "      <td>0.939830</td>\n",
       "      <td>-6.263000e+01</td>\n",
       "      <td>-0.017365</td>\n",
       "      <td>-0.024002</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>1.004625</td>\n",
       "      <td>0.227178</td>\n",
       "      <td>...</td>\n",
       "      <td>-101.035000</td>\n",
       "      <td>-0.004599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.295100</td>\n",
       "      <td>4.497000</td>\n",
       "      <td>4.608400e+01</td>\n",
       "      <td>2.710400</td>\n",
       "      <td>2.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.030163</td>\n",
       "      <td>0.533995</td>\n",
       "      <td>0.150895</td>\n",
       "      <td>1.389600</td>\n",
       "      <td>-1.248800e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036338</td>\n",
       "      <td>0.828075</td>\n",
       "      <td>1.174700</td>\n",
       "      <td>0.444560</td>\n",
       "      <td>...</td>\n",
       "      <td>648.790000</td>\n",
       "      <td>0.041653</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.961460</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>9.505600</td>\n",
       "      <td>6.746300</td>\n",
       "      <td>8.079100e+01</td>\n",
       "      <td>4.487400</td>\n",
       "      <td>4.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.110607</td>\n",
       "      <td>0.758385</td>\n",
       "      <td>0.368032</td>\n",
       "      <td>2.427500</td>\n",
       "      <td>3.669300e+01</td>\n",
       "      <td>0.030619</td>\n",
       "      <td>0.128873</td>\n",
       "      <td>2.107750</td>\n",
       "      <td>2.075975</td>\n",
       "      <td>0.664230</td>\n",
       "      <td>...</td>\n",
       "      <td>3757.500000</td>\n",
       "      <td>0.117140</td>\n",
       "      <td>0.279310</td>\n",
       "      <td>1.001500</td>\n",
       "      <td>0.252730</td>\n",
       "      <td>20.027500</td>\n",
       "      <td>10.988500</td>\n",
       "      <td>1.334850e+02</td>\n",
       "      <td>7.876600</td>\n",
       "      <td>9.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.481000</td>\n",
       "      <td>480.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6845.800000</td>\n",
       "      <td>9.909000e+05</td>\n",
       "      <td>77.488000</td>\n",
       "      <td>20.481000</td>\n",
       "      <td>53209.000000</td>\n",
       "      <td>136.050000</td>\n",
       "      <td>266.860000</td>\n",
       "      <td>...</td>\n",
       "      <td>716160.000000</td>\n",
       "      <td>112.020000</td>\n",
       "      <td>147.190000</td>\n",
       "      <td>59672.000000</td>\n",
       "      <td>7617.300000</td>\n",
       "      <td>203500.000000</td>\n",
       "      <td>21110.000000</td>\n",
       "      <td>7.276000e+06</td>\n",
       "      <td>7641.300000</td>\n",
       "      <td>143380.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Attr1        Attr2        Attr3        Attr4         Attr5  \\\n",
       "count  6090.000000  6090.000000  6090.000000  6081.000000  6.081000e+03   \n",
       "mean     -0.080979     0.924284    -0.192666     5.525081 -1.594755e+02   \n",
       "std       6.081010    11.668831    11.635570    93.418738  1.962969e+04   \n",
       "min    -463.890000     0.000000  -479.960000    -0.403110 -1.076400e+06   \n",
       "25%      -0.023993     0.312793    -0.031263     0.939830 -6.263000e+01   \n",
       "50%       0.030163     0.533995     0.150895     1.389600 -1.248800e+01   \n",
       "75%       0.110607     0.758385     0.368032     2.427500  3.669300e+01   \n",
       "max      20.481000   480.960000     1.000000  6845.800000  9.909000e+05   \n",
       "\n",
       "             Attr6        Attr7         Attr8        Attr9       Attr10  ...  \\\n",
       "count  6090.000000  6090.000000   6084.000000  6090.000000  6090.000000  ...   \n",
       "mean     -0.472636    -0.067617     19.953860     1.811503     0.122745  ...   \n",
       "std      13.186012     6.083108    747.706288     3.136768    12.230470  ...   \n",
       "min    -508.410000  -463.890000     -2.003200    -3.496000  -479.910000  ...   \n",
       "25%      -0.017365    -0.024002      0.301300     1.004625     0.227178  ...   \n",
       "50%       0.000000     0.036338      0.828075     1.174700     0.444560  ...   \n",
       "75%       0.030619     0.128873      2.107750     2.075975     0.664230  ...   \n",
       "max      77.488000    20.481000  53209.000000   136.050000   266.860000  ...   \n",
       "\n",
       "              Attr55       Attr56       Attr57        Attr58       Attr59  \\\n",
       "count    6091.000000  6071.000000  6089.000000   6077.000000  6089.000000   \n",
       "mean     5082.324856    -1.433530    -0.603328     13.585445     1.979408   \n",
       "std     34362.636399    97.328136    28.044677    790.781341   100.666203   \n",
       "min   -311100.000000 -7522.100000 -1667.300000     -0.085920  -256.990000   \n",
       "25%      -101.035000    -0.004599     0.000000      0.886040     0.000000   \n",
       "50%       648.790000     0.041653     0.099789      0.961460     0.001473   \n",
       "75%      3757.500000     0.117140     0.279310      1.001500     0.252730   \n",
       "max    716160.000000   112.020000   147.190000  59672.000000  7617.300000   \n",
       "\n",
       "              Attr60        Attr61        Attr62       Attr63         Attr64  \n",
       "count    5730.000000   6075.000000  6.071000e+03  6081.000000    5933.000000  \n",
       "mean      187.357968     17.000457  1.837821e+03    10.062505      82.390377  \n",
       "std      4256.876238    275.865970  9.481545e+04   105.798150    2539.423334  \n",
       "min         0.000000     -0.003150 -1.496500e+04    -1.543200      -0.000015  \n",
       "25%         5.295100      4.497000  4.608400e+01     2.710400       2.102100  \n",
       "50%         9.505600      6.746300  8.079100e+01     4.487400       4.256300  \n",
       "75%        20.027500     10.988500  1.334850e+02     7.876600       9.793200  \n",
       "max    203500.000000  21110.000000  7.276000e+06  7641.300000  143380.000000  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73a2dd",
   "metadata": {},
   "source": [
    "# We check for NAN values and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab77023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr1        1\n",
      "Attr2        1\n",
      "Attr3        1\n",
      "Attr4       10\n",
      "Attr5       10\n",
      "Attr6        1\n",
      "Attr7        1\n",
      "Attr8        7\n",
      "Attr9        1\n",
      "Attr10       1\n",
      "Attr11      37\n",
      "Attr12      10\n",
      "Attr13      20\n",
      "Attr14       1\n",
      "Attr15       0\n",
      "Attr16       7\n",
      "Attr17       7\n",
      "Attr18       1\n",
      "Attr19      20\n",
      "Attr20      20\n",
      "Attr21    1121\n",
      "Attr22       1\n",
      "Attr23      20\n",
      "Attr24     106\n",
      "Attr25       1\n",
      "Attr26       7\n",
      "Attr27     898\n",
      "Attr28     158\n",
      "Attr29       1\n",
      "Attr30      20\n",
      "Attr31      20\n",
      "Attr32      47\n",
      "Attr33      10\n",
      "Attr34       7\n",
      "Attr35       1\n",
      "Attr36       1\n",
      "Attr37    2735\n",
      "Attr38       1\n",
      "Attr39      20\n",
      "Attr40      10\n",
      "Attr41      65\n",
      "Attr42      20\n",
      "Attr43      20\n",
      "Attr44      20\n",
      "Attr45     359\n",
      "Attr46      11\n",
      "Attr47      26\n",
      "Attr48       1\n",
      "Attr49      20\n",
      "Attr50       7\n",
      "Attr51       1\n",
      "Attr52      27\n",
      "Attr53     158\n",
      "Attr54     158\n",
      "Attr55       0\n",
      "Attr56      20\n",
      "Attr57       2\n",
      "Attr58      14\n",
      "Attr59       2\n",
      "Attr60     361\n",
      "Attr61      16\n",
      "Attr62      20\n",
      "Attr63      10\n",
      "Attr64     158\n",
      "class        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25287ba6",
   "metadata": {},
   "source": [
    "## Except for the ATTR 21  ATTR 37 and ATTR 27, the number of NAN are reasonable. So we're gonna replace all the NAN in the others olumns by the mean of the columns. For the 21 ans 37, we're gonna replace NAN by \"No\". Maybe this data will be useful four the machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a56e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr1        0\n",
      "Attr2        0\n",
      "Attr3        0\n",
      "Attr4        0\n",
      "Attr5        0\n",
      "Attr6        0\n",
      "Attr7        0\n",
      "Attr8        0\n",
      "Attr9        0\n",
      "Attr10       0\n",
      "Attr11       0\n",
      "Attr12       0\n",
      "Attr13       0\n",
      "Attr14       0\n",
      "Attr15       0\n",
      "Attr16       0\n",
      "Attr17       0\n",
      "Attr18       0\n",
      "Attr19       0\n",
      "Attr20       0\n",
      "Attr21    1121\n",
      "Attr22       0\n",
      "Attr23       0\n",
      "Attr24       0\n",
      "Attr25       0\n",
      "Attr26       0\n",
      "Attr27     898\n",
      "Attr28       0\n",
      "Attr29       0\n",
      "Attr30       0\n",
      "Attr31       0\n",
      "Attr32       0\n",
      "Attr33       0\n",
      "Attr34       0\n",
      "Attr35       0\n",
      "Attr36       0\n",
      "Attr37    2735\n",
      "Attr38       0\n",
      "Attr39       0\n",
      "Attr40       0\n",
      "Attr41       0\n",
      "Attr42       0\n",
      "Attr43       0\n",
      "Attr44       0\n",
      "Attr45       0\n",
      "Attr46       0\n",
      "Attr47       0\n",
      "Attr48       0\n",
      "Attr49       0\n",
      "Attr50       0\n",
      "Attr51       0\n",
      "Attr52       0\n",
      "Attr53       0\n",
      "Attr54       0\n",
      "Attr55       0\n",
      "Attr56       0\n",
      "Attr57       0\n",
      "Attr58       0\n",
      "Attr59       0\n",
      "Attr60       0\n",
      "Attr61       0\n",
      "Attr62       0\n",
      "Attr63       0\n",
      "Attr64       0\n",
      "class        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns : \n",
    "    if(column != 'Attr21' and column != 'Attr37' and column != 'Attr27'):\n",
    "        a = np.array(df[column])\n",
    "        mean = np.nanmean(a,dtype='float32')\n",
    "        df[column] = df[column].replace(np.nan,mean) \n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa48cf",
   "metadata": {},
   "source": [
    "## We use the KNN imputer to replace the NAN value. Before we're gonna scale the dataset (without the target of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c42f6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "25    0\n",
      "26    0\n",
      "27    0\n",
      "28    0\n",
      "29    0\n",
      "30    0\n",
      "31    0\n",
      "32    0\n",
      "33    0\n",
      "34    0\n",
      "35    0\n",
      "36    0\n",
      "37    0\n",
      "38    0\n",
      "39    0\n",
      "40    0\n",
      "41    0\n",
      "42    0\n",
      "43    0\n",
      "44    0\n",
      "45    0\n",
      "46    0\n",
      "47    0\n",
      "48    0\n",
      "49    0\n",
      "50    0\n",
      "51    0\n",
      "52    0\n",
      "53    0\n",
      "54    0\n",
      "55    0\n",
      "56    0\n",
      "57    0\n",
      "58    0\n",
      "59    0\n",
      "60    0\n",
      "61    0\n",
      "62    0\n",
      "63    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.957865</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.520686</td>\n",
       "      <td>0.867549</td>\n",
       "      <td>0.957900</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.081269</td>\n",
       "      <td>0.642994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303267</td>\n",
       "      <td>0.985327</td>\n",
       "      <td>0.919034</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.032901</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957857</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.998688</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.520783</td>\n",
       "      <td>0.868172</td>\n",
       "      <td>0.957905</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.030366</td>\n",
       "      <td>0.643623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302847</td>\n",
       "      <td>0.985400</td>\n",
       "      <td>0.918881</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.032639</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.957953</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.998415</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>0.958008</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.038253</td>\n",
       "      <td>0.643396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303657</td>\n",
       "      <td>0.985336</td>\n",
       "      <td>0.918994</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.956971</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.998838</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.520699</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>0.956978</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0.643455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306203</td>\n",
       "      <td>0.985297</td>\n",
       "      <td>0.918551</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.956591</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.997401</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.520662</td>\n",
       "      <td>0.867477</td>\n",
       "      <td>0.956591</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.051234</td>\n",
       "      <td>0.642890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302826</td>\n",
       "      <td>0.985308</td>\n",
       "      <td>0.917218</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>0.957754</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.997006</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.520637</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>0.957788</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.038731</td>\n",
       "      <td>0.642779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302523</td>\n",
       "      <td>0.985332</td>\n",
       "      <td>0.918982</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.640295</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.997068</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.520597</td>\n",
       "      <td>0.867615</td>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.032209</td>\n",
       "      <td>0.642709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269882</td>\n",
       "      <td>0.985334</td>\n",
       "      <td>0.916142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.033175</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>0.957380</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.520657</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>0.957252</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.040604</td>\n",
       "      <td>0.643001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302945</td>\n",
       "      <td>0.985314</td>\n",
       "      <td>0.918540</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.032702</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>0.958557</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.520694</td>\n",
       "      <td>0.868113</td>\n",
       "      <td>0.958557</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.044152</td>\n",
       "      <td>0.643770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305274</td>\n",
       "      <td>0.985345</td>\n",
       "      <td>0.919148</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>0.957468</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.998512</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.520658</td>\n",
       "      <td>0.866901</td>\n",
       "      <td>0.957376</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.037904</td>\n",
       "      <td>0.642393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313143</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.919229</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.032123</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4872 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.957865  0.001542  0.999008  0.000578  0.520686  0.867549  0.957900   \n",
       "1     0.957857  0.000565  0.998688  0.000415  0.520783  0.868172  0.957905   \n",
       "2     0.957953  0.000917  0.998415  0.000298  0.520700  0.867745  0.958008   \n",
       "3     0.956971  0.000825  0.998838  0.000384  0.520699  0.867745  0.956978   \n",
       "4     0.956591  0.001703  0.997401  0.000160  0.520662  0.867477  0.956591   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4867  0.957754  0.001875  0.997006  0.000133  0.520637  0.867745  0.957788   \n",
       "4868  0.957245  0.001984  0.997068  0.000118  0.520597  0.867615  0.957245   \n",
       "4869  0.957380  0.001531  0.998147  0.000232  0.520657  0.867745  0.957252   \n",
       "4870  0.958557  0.000336  0.998853  0.000610  0.520694  0.868113  0.958557   \n",
       "4871  0.957468  0.002475  0.998512  0.000323  0.520658  0.866901  0.957376   \n",
       "\n",
       "            7         8         9   ...        54        55        56  \\\n",
       "0     0.000044  0.081269  0.642994  ...  0.303267  0.985327  0.919034   \n",
       "1     0.000088  0.030366  0.643623  ...  0.302847  0.985400  0.918881   \n",
       "2     0.000061  0.038253  0.643396  ...  0.303657  0.985336  0.918994   \n",
       "3     0.000066  0.037579  0.643455  ...  0.306203  0.985297  0.918551   \n",
       "4     0.000042  0.051234  0.642890  ...  0.302826  0.985308  0.917218   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4867  0.000040  0.038731  0.642779  ...  0.302523  0.985332  0.918982   \n",
       "4868  0.000039  0.032209  0.642709  ...  0.269882  0.985334  0.916142   \n",
       "4869  0.000044  0.040604  0.643001  ...  0.302945  0.985314  0.918540   \n",
       "4870  0.000135  0.044152  0.643770  ...  0.305274  0.985345  0.919148   \n",
       "4871  0.000035  0.037904  0.642393  ...  0.313143  0.985337  0.919229   \n",
       "\n",
       "            57        58        59        60        61        62        63  \n",
       "0     0.000018  0.032901  0.000165  0.001226  0.002054  0.005222  0.000201  \n",
       "1     0.000010  0.032639  0.000046  0.000093  0.002070  0.000580  0.000014  \n",
       "2     0.000017  0.032637  0.000254  0.000152  0.002063  0.000847  0.000033  \n",
       "3     0.000021  0.032637  0.000044  0.000138  0.002063  0.000836  0.000062  \n",
       "4     0.000021  0.032637  0.000128  0.000428  0.002064  0.000785  0.000059  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4867  0.000018  0.032637  0.640295  0.000377  0.002076  0.000480  0.000025  \n",
       "4868  0.000019  0.033175  0.000037  0.000427  0.002087  0.000393  0.000010  \n",
       "4869  0.000020  0.032702  0.000030  0.000371  0.002066  0.000678  0.000051  \n",
       "4870  0.000016  0.032637  0.000051  0.000447  0.002056  0.002357  0.000048  \n",
       "4871  0.000020  0.032123  0.000019  0.000588  0.002062  0.000868  0.000034  \n",
       "\n",
       "[4872 rows x 64 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df['Attr27'].replace(np.nan,'NO',inplace = True)\n",
    "df['Attr37'].replace(np.nan,'NO',inplace = True)\n",
    "df['Attr21'].replace(np.nan,'NO',inplace = True)\n",
    "\n",
    "print(values_class)\n",
    "df0 = df[(df['class'] == values_class[0]) & (df['Attr27'] != 'NO')]\n",
    "df1 = df[(df['class'] == values_class[1])& (df['Attr27'] != 'NO')]\n",
    "print(((df0['Attr27']).mean(),(df1['Attr27']).mean()))\n",
    "print(((df0['Attr27']).var(),(df1['Attr27']).var()))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "df0 = df[(df['class'] == values_class[0]) & (df['Attr37'] != 'NO')]\n",
    "df1 = df[(df['class'] == values_class[1])& (df['Attr37'] != 'NO')]\n",
    "print(((df0['Attr37']).mean(),(df1['Attr37']).mean()))\n",
    "print(((df0['Attr37']).var(),(df1['Attr37']).var()))\n",
    "print(\"\\n\")\n",
    "\n",
    "      \n",
    "df0 = df[(df['class'] == values_class[0]) & (df['Attr21'] != 'NO')]\n",
    "df1 = df[(df['class'] == values_class[1])& (df['Attr21'] != 'NO')]\n",
    "print(((df0['Attr21']).mean(),(df1['Attr21']).mean()))\n",
    "print(((df0['Attr21']).var(),(df1['Attr21']).var()))\n",
    "print(\"\\n\")\n",
    "'''\n",
    "X = df.drop(['class'],axis = 1)\n",
    "Y = df['class']\n",
    "scaler = MinMaxScaler().fit(X)\n",
    "scaled_X = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_X, Y, test_size = 0.2, random_state = 5)\n",
    "(X_train,X_test)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(imputer.fit_transform(X_test))\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(pd.DataFrame(X_train).isna().sum())\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d648c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=Y_train.astype('int')\n",
    "Y = Y.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d24fd",
   "metadata": {},
   "source": [
    "# Now , we check for duplicates and if it exist, we drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9be03c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd126208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace = True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319186b",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "## At the end , we want the user to use an api to test his data. The API will not be useful with this much features. So we're gonna use 2 features selection methods and keep the features with the most importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51b89a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01864456 0.02309668 0.02724089 0.01263417 0.01301013 0.01691607\n",
      " 0.01741657 0.01382249 0.01447807 0.01835321 0.01948804 0.01289371\n",
      " 0.01443066 0.01930228 0.0149303  0.01473748 0.01292935 0.01705932\n",
      " 0.01167086 0.01574205 0.01337025 0.02022987 0.01137171 0.02426502\n",
      " 0.01884985 0.0154745  0.01430483 0.01235869 0.02145951 0.01195173\n",
      " 0.01238386 0.01239492 0.01229916 0.01595449 0.02604802 0.01368288\n",
      " 0.01285823 0.0217349  0.01649245 0.01389575 0.01325337 0.01308702\n",
      " 0.01153801 0.01406562 0.01176812 0.02709635 0.01392218 0.01423873\n",
      " 0.01165962 0.01397309 0.02967243 0.01116748 0.01164061 0.01256218\n",
      " 0.01989292 0.0143163  0.01360144 0.01579315 0.01201599 0.01343846\n",
      " 0.01414892 0.01211352 0.01270116 0.01215582]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train,Y_train)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cb751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dbd8a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False],\n",
       " [23, 40, 31, 24, 50, 2, 5, 1, 54, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cor_selector(X, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = X.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "cor_support, cor_feature = cor_selector(pd.DataFrame(X_train), Y_train,10)\n",
    "(cor_support, cor_feature) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c298fbe",
   "metadata": {},
   "source": [
    "# So we have run the algorithms several times, the attributes that came up often was \n",
    "\n",
    "## 27 : profit on operating activities / financial expenses. \n",
    "## 56 : (sales - cost of products sold) / sales \n",
    "## 1 : net profit / total assets \n",
    "## 2 : total liabilities / total assets \n",
    "## 35 : profit on sales / total assets \n",
    "\n",
    "# We'll keep this attributes to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "624152b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.957865</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.520686</td>\n",
       "      <td>0.867549</td>\n",
       "      <td>0.957900</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.081269</td>\n",
       "      <td>0.642994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303267</td>\n",
       "      <td>0.985327</td>\n",
       "      <td>0.919034</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.032901</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957857</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.998688</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.520783</td>\n",
       "      <td>0.868172</td>\n",
       "      <td>0.957905</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.030366</td>\n",
       "      <td>0.643623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302847</td>\n",
       "      <td>0.985400</td>\n",
       "      <td>0.918881</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.032639</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.957953</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.998415</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>0.958008</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.038253</td>\n",
       "      <td>0.643396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303657</td>\n",
       "      <td>0.985336</td>\n",
       "      <td>0.918994</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.956971</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.998838</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.520699</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>0.956978</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0.643455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306203</td>\n",
       "      <td>0.985297</td>\n",
       "      <td>0.918551</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.956591</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.997401</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.520662</td>\n",
       "      <td>0.867477</td>\n",
       "      <td>0.956591</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.051234</td>\n",
       "      <td>0.642890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302826</td>\n",
       "      <td>0.985308</td>\n",
       "      <td>0.917218</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>0.957754</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.997006</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.520637</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>0.957788</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.038731</td>\n",
       "      <td>0.642779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302523</td>\n",
       "      <td>0.985332</td>\n",
       "      <td>0.918982</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.640295</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.997068</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.520597</td>\n",
       "      <td>0.867615</td>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.032209</td>\n",
       "      <td>0.642709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269882</td>\n",
       "      <td>0.985334</td>\n",
       "      <td>0.916142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.033175</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>0.957380</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.520657</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>0.957252</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.040604</td>\n",
       "      <td>0.643001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302945</td>\n",
       "      <td>0.985314</td>\n",
       "      <td>0.918540</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.032702</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>0.958557</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.520694</td>\n",
       "      <td>0.868113</td>\n",
       "      <td>0.958557</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.044152</td>\n",
       "      <td>0.643770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305274</td>\n",
       "      <td>0.985345</td>\n",
       "      <td>0.919148</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>0.957468</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.998512</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.520658</td>\n",
       "      <td>0.866901</td>\n",
       "      <td>0.957376</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.037904</td>\n",
       "      <td>0.642393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313143</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.919229</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.032123</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4872 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.957865  0.001542  0.999008  0.000578  0.520686  0.867549  0.957900   \n",
       "1     0.957857  0.000565  0.998688  0.000415  0.520783  0.868172  0.957905   \n",
       "2     0.957953  0.000917  0.998415  0.000298  0.520700  0.867745  0.958008   \n",
       "3     0.956971  0.000825  0.998838  0.000384  0.520699  0.867745  0.956978   \n",
       "4     0.956591  0.001703  0.997401  0.000160  0.520662  0.867477  0.956591   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4867  0.957754  0.001875  0.997006  0.000133  0.520637  0.867745  0.957788   \n",
       "4868  0.957245  0.001984  0.997068  0.000118  0.520597  0.867615  0.957245   \n",
       "4869  0.957380  0.001531  0.998147  0.000232  0.520657  0.867745  0.957252   \n",
       "4870  0.958557  0.000336  0.998853  0.000610  0.520694  0.868113  0.958557   \n",
       "4871  0.957468  0.002475  0.998512  0.000323  0.520658  0.866901  0.957376   \n",
       "\n",
       "            7         8         9   ...        54        55        56  \\\n",
       "0     0.000044  0.081269  0.642994  ...  0.303267  0.985327  0.919034   \n",
       "1     0.000088  0.030366  0.643623  ...  0.302847  0.985400  0.918881   \n",
       "2     0.000061  0.038253  0.643396  ...  0.303657  0.985336  0.918994   \n",
       "3     0.000066  0.037579  0.643455  ...  0.306203  0.985297  0.918551   \n",
       "4     0.000042  0.051234  0.642890  ...  0.302826  0.985308  0.917218   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4867  0.000040  0.038731  0.642779  ...  0.302523  0.985332  0.918982   \n",
       "4868  0.000039  0.032209  0.642709  ...  0.269882  0.985334  0.916142   \n",
       "4869  0.000044  0.040604  0.643001  ...  0.302945  0.985314  0.918540   \n",
       "4870  0.000135  0.044152  0.643770  ...  0.305274  0.985345  0.919148   \n",
       "4871  0.000035  0.037904  0.642393  ...  0.313143  0.985337  0.919229   \n",
       "\n",
       "            57        58        59        60        61        62        63  \n",
       "0     0.000018  0.032901  0.000165  0.001226  0.002054  0.005222  0.000201  \n",
       "1     0.000010  0.032639  0.000046  0.000093  0.002070  0.000580  0.000014  \n",
       "2     0.000017  0.032637  0.000254  0.000152  0.002063  0.000847  0.000033  \n",
       "3     0.000021  0.032637  0.000044  0.000138  0.002063  0.000836  0.000062  \n",
       "4     0.000021  0.032637  0.000128  0.000428  0.002064  0.000785  0.000059  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4867  0.000018  0.032637  0.640295  0.000377  0.002076  0.000480  0.000025  \n",
       "4868  0.000019  0.033175  0.000037  0.000427  0.002087  0.000393  0.000010  \n",
       "4869  0.000020  0.032702  0.000030  0.000371  0.002066  0.000678  0.000051  \n",
       "4870  0.000016  0.032637  0.000051  0.000447  0.002056  0.002357  0.000048  \n",
       "4871  0.000020  0.032123  0.000019  0.000588  0.002062  0.000868  0.000034  \n",
       "\n",
       "[4872 rows x 64 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0be1206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_profit</th>\n",
       "      <th>total_liabilities</th>\n",
       "      <th>profit_operating_activities</th>\n",
       "      <th>profit_on_sales</th>\n",
       "      <th>sales_minus_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.957865</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.985327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957857</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.112795</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.957953</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.112806</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.985336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.956971</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.112759</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.985297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.956591</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.112789</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>0.985308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>0.957754</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.112796</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.985332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.985334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>0.957380</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.112789</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.985314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>0.958557</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.112880</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.985345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>0.957468</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.112794</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.985337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4872 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_profit  total_liabilities  profit_operating_activities  \\\n",
       "0       0.957865           0.001542                     0.114400   \n",
       "1       0.957857           0.000565                     0.112795   \n",
       "2       0.957953           0.000917                     0.112806   \n",
       "3       0.956971           0.000825                     0.112759   \n",
       "4       0.956591           0.001703                     0.112789   \n",
       "...          ...                ...                          ...   \n",
       "4867    0.957754           0.001875                     0.112796   \n",
       "4868    0.957245           0.001984                     0.112793   \n",
       "4869    0.957380           0.001531                     0.112789   \n",
       "4870    0.958557           0.000336                     0.112880   \n",
       "4871    0.957468           0.002475                     0.112794   \n",
       "\n",
       "      profit_on_sales  sales_minus_cost  \n",
       "0            0.008176          0.985327  \n",
       "1            0.000768          0.985400  \n",
       "2            0.001908          0.985336  \n",
       "3            0.001871          0.985297  \n",
       "4            0.003783          0.985308  \n",
       "...               ...               ...  \n",
       "4867         0.001977          0.985332  \n",
       "4868         0.001077          0.985334  \n",
       "4869         0.002247          0.985314  \n",
       "4870         0.002760          0.985345  \n",
       "4871         0.001857          0.985337  \n",
       "\n",
       "[4872 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_keep = [0,1,26,35,55]\n",
    "X_train = X_train[columns_to_keep]\n",
    "X_test = X_test[columns_to_keep]\n",
    "print(len(X_train.columns))\n",
    "X_train.columns = [\"net_profit\",\"total_liabilities\",\"profit_operating_activities\",\"profit_on_sales\",\"sales_minus_cost\"]\n",
    "X_test.columns = [\"net_profit\",\"total_liabilities\",\"profit_operating_activities\",\"profit_on_sales\",\"sales_minus_cost\"]\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8534b",
   "metadata": {},
   "source": [
    "# Now , we'll do some Data visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46951e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.patches.Wedge at 0x18e1130ad30>,\n",
       "  <matplotlib.patches.Wedge at 0x18e112f3100>],\n",
       " [Text(0.5001477583170267, 0.979720480469023, '1'),\n",
       "  Text(-0.5001476665889737, -0.979720527296231, '0')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_class = []\n",
    "\n",
    "somme = 0\n",
    "for i in Y_train:\n",
    "    if(i == 1):\n",
    "        somme += 1 \n",
    "        \n",
    "number_class.append(somme)\n",
    "number_class.append(Y_train.size - somme)\n",
    "\n",
    "label = [\"1\",\"0\"]\n",
    "plt.pie(number_class,labels = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00624787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_profit</th>\n",
       "      <th>total_liabilities</th>\n",
       "      <th>profit_operating_activities</th>\n",
       "      <th>profit_on_sales</th>\n",
       "      <th>sales_minus_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.957865</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.985327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957857</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.112795</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.957953</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.112806</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.985336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.956971</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.112759</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.985297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.956591</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.112789</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>0.985308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>0.957754</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.112796</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.985332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.985334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>0.957380</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.112789</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.985314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>0.958557</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.112880</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.985345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>0.957468</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.112794</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.985337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4872 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_profit  total_liabilities  profit_operating_activities  \\\n",
       "0       0.957865           0.001542                     0.114400   \n",
       "1       0.957857           0.000565                     0.112795   \n",
       "2       0.957953           0.000917                     0.112806   \n",
       "3       0.956971           0.000825                     0.112759   \n",
       "4       0.956591           0.001703                     0.112789   \n",
       "...          ...                ...                          ...   \n",
       "4867    0.957754           0.001875                     0.112796   \n",
       "4868    0.957245           0.001984                     0.112793   \n",
       "4869    0.957380           0.001531                     0.112789   \n",
       "4870    0.958557           0.000336                     0.112880   \n",
       "4871    0.957468           0.002475                     0.112794   \n",
       "\n",
       "      profit_on_sales  sales_minus_cost  \n",
       "0            0.008176          0.985327  \n",
       "1            0.000768          0.985400  \n",
       "2            0.001908          0.985336  \n",
       "3            0.001871          0.985297  \n",
       "4            0.003783          0.985308  \n",
       "...               ...               ...  \n",
       "4867         0.001977          0.985332  \n",
       "4868         0.001077          0.985334  \n",
       "4869         0.002247          0.985314  \n",
       "4870         0.002760          0.985345  \n",
       "4871         0.001857          0.985337  \n",
       "\n",
       "[4872 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53edbe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01445150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nfrom xgboost import XGBRegressor\\nxgb_model = XGBRegressor(random_state = 2021)\\n\\n\\n# make a dictionary of hyperparameter values to search\\nsearch_space = {\\n    \"n_estimators\" : [100, 200, 500],\\n    \"max_depth\" : [3, 6, 9],\\n    \"gamma\" : [0.01, 0.1],\\n    \"learning_rate\" : [0.001, 0.01, 0.1, 1]\\n}\\n\\nfrom sklearn.model_selection import GridSearchCV\\n# make a GridSearchCV object\\nGS = GridSearchCV(estimator = xgb_model,\\n                  param_grid = search_space,\\n                  scoring = [\"r2\", \"neg_root_mean_squared_error\"], #sklearn.metrics.SCORERS.keys()\\n                  refit = \"r2\",\\n                  cv = 5,\\n                  verbose = 4)\\n\\na = GS.fit(X_train, Y_train)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "xgb_model = XGBRegressor(random_state = 2021)\n",
    "\n",
    "\n",
    "# make a dictionary of hyperparameter values to search\n",
    "search_space = {\n",
    "    \"n_estimators\" : [100, 200, 500],\n",
    "    \"max_depth\" : [3, 6, 9],\n",
    "    \"gamma\" : [0.01, 0.1],\n",
    "    \"learning_rate\" : [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# make a GridSearchCV object\n",
    "GS = GridSearchCV(estimator = xgb_model,\n",
    "                  param_grid = search_space,\n",
    "                  scoring = [\"r2\", \"neg_root_mean_squared_error\"], #sklearn.metrics.SCORERS.keys()\n",
    "                  refit = \"r2\",\n",
    "                  cv = 5,\n",
    "                  verbose = 4)\n",
    "\n",
    "a = GS.fit(X_train, Y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91469689",
   "metadata": {},
   "source": [
    "# XGB BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d317785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# A parameter grid for XGBoost\\nestimator = xgb.XGBClassifier(\\n    objective= 'binary:logistic',\\n    nthread=4,\\n    seed=42\\n)\\n\\nparameters = {\\n    'max_depth': range (2, 10, 1),\\n    'n_estimators': range(60, 220, 40),\\n    'learning_rate': [0.1, 0.01, 0.05]\\n}\\n\\ngrid_search = GridSearchCV(\\n    estimator=estimator,\\n    param_grid=parameters,\\n    scoring = 'roc_auc',\\n    n_jobs = 10,\\n    cv = 10,\\n    verbose=True\\n)\\n\\n\\ngrid_search.fit(X, Y)\\n\\ngrid_search.best_estimator_\\n\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# A parameter grid for XGBoost\n",
    "estimator = xgb.XGBClassifier(\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': range(60, 220, 40),\n",
    "    'learning_rate': [0.1, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=parameters,\n",
    "    scoring = 'roc_auc',\n",
    "    n_jobs = 10,\n",
    "    cv = 10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "grid_search.best_estimator_\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3efe2",
   "metadata": {},
   "source": [
    "# The code took to much time to execute, so we're gonna set the XG boost classifier with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80cbb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best estimator:\n",
    "## XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bytree=0.6, gamma=5, learning_rate=0.02, max_delta_step=0,\n",
    "## max_depth=5, min_child_weight=5, missing=None, n_estimators=600,\n",
    "## n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
    "## reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb51ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THE CLASSIFIER WITH THIS PARAM DOESN'T WORK AT THE END\n",
    "xgb_classifier = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.6, gamma=5, learning_rate=0.02, max_delta_step=0,\n",
    "       max_depth=5, min_child_weight=5, missing=None, n_estimators=600,\n",
    "       n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1.0,verbose  = 0,logging_level = 'Silent')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244b367",
   "metadata": {},
   "source": [
    "# CATBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c4ebbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CBC = CatBoostClassifier()\\n\\nparameters = {\\'depth\\'         : [4,5,6,7,8,9, 10],\\n                 \\'learning_rate\\' : [0.01,0.02,0.03,0.04],\\n                  \\'iterations\\'    : [10, 20,30,40,50,60,70,80,90, 100]\\n                 }\\n    \\nGrid_CBC = GridSearchCV(estimator=CBC, param_grid = parameters, cv = 2, n_jobs=-1)\\nGrid_CBC.fit(X_train, Y_train)\\nprint(\" Results from Grid Search \" )\\nprint(\"\\n The best estimator across ALL searched params:\\n\",Grid_CBC.best_estimator_)\\nprint(\"\\n The best score across ALL searched params:\\n\",Grid_CBC.best_score_)\\nprint(\"\\n The best parameters across ALL searched params:\\n\",Grid_CBC.best_params_)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''CBC = CatBoostClassifier()\n",
    "\n",
    "parameters = {'depth'         : [4,5,6,7,8,9, 10],\n",
    "                 'learning_rate' : [0.01,0.02,0.03,0.04],\n",
    "                  'iterations'    : [10, 20,30,40,50,60,70,80,90, 100]\n",
    "                 }\n",
    "    \n",
    "Grid_CBC = GridSearchCV(estimator=CBC, param_grid = parameters, cv = 2, n_jobs=-1)\n",
    "Grid_CBC.fit(X_train, Y_train)\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",Grid_CBC.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",Grid_CBC.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",Grid_CBC.best_params_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e449b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best parameters across ALL searched params: {'depth': 9, 'iterations': 100, 'learning_rate': 0.04}\n",
    "CBC = CatBoostClassifier(depth = 9, iterations = 100, learning_rate = 0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d800fabc",
   "metadata": {},
   "source": [
    "# SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d158672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\\ngrid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\\ngrid.fit(X_train,Y_train)\\ngrid.best_params_\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "grid.fit(X_train,Y_train)\n",
    "grid.best_params_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f57dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best parameters across ALL searched params: {C = 1000, gamma = 1, kernel = 'rbf'),}\n",
    "SVC_classifier = SVC(C = 1000, gamma = 1, kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf3322",
   "metadata": {},
   "source": [
    "# MLP CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba4c28c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mlp_gs = MLPClassifier(max_iter=100)\\nparameter_space = {\\n    'hidden_layer_sizes': [(10,30,10),(20,)],\\n    'activation': ['tanh', 'relu'],\\n    'solver': ['sgd', 'adam'],\\n    'alpha': [0.0001, 0.05],\\n    'learning_rate': ['constant','adaptive'],\\n}\\n\\nclf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\\nclf.fit(X_train, Y_train) # X is train samples and y is the corresponding labels\\nprint('Best parameters found:\\n', clf.best_params_)\\n\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mlp_gs = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, Y_train) # X is train samples and y is the corresponding labels\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "268428c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAM =  {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
    "MLP_classifier = MLPClassifier(activation = 'tanh',alpha = 0.001, hidden_layer_sizes = (10,30,10),learning_rate = 'constant',solver = 'sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0979f",
   "metadata": {},
   "source": [
    "# GAUSSIAN NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1398a528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"param_grid_nb = {\\n    'var_smoothing': np.logspace(0,-9, num=100)\\n}\\n\\nnbModel_grid = GridSearchCV(estimator=GaussianNB(), \\n                            param_grid=param_grid_nb, \\n                            verbose=1, \\n                            cv=10, \\n                            n_jobs=-1)\\nnbModel_grid.fit(X_train, Y_train)\\nprint(nbModel_grid.best_estimator_)\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "\n",
    "nbModel_grid = GridSearchCV(estimator=GaussianNB(), \n",
    "                            param_grid=param_grid_nb, \n",
    "                            verbose=1, \n",
    "                            cv=10, \n",
    "                            n_jobs=-1)\n",
    "nbModel_grid.fit(X_train, Y_train)\n",
    "print(nbModel_grid.best_estimator_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e65428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAM = var_smoothing=1.0\n",
    "Gaussian_NB = GaussianNB(var_smoothing = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "344ce486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "CatBoost\n",
      "0:\tlearn: 0.6602590\ttotal: 64.5ms\tremaining: 6.38s\n",
      "1:\tlearn: 0.6374501\ttotal: 127ms\tremaining: 6.2s\n",
      "2:\tlearn: 0.6113662\ttotal: 192ms\tremaining: 6.2s\n",
      "3:\tlearn: 0.5907091\ttotal: 255ms\tremaining: 6.11s\n",
      "4:\tlearn: 0.5669344\ttotal: 314ms\tremaining: 5.96s\n",
      "5:\tlearn: 0.5463457\ttotal: 375ms\tremaining: 5.87s\n",
      "6:\tlearn: 0.5293294\ttotal: 440ms\tremaining: 5.84s\n",
      "7:\tlearn: 0.5132156\ttotal: 499ms\tremaining: 5.73s\n",
      "8:\tlearn: 0.4970144\ttotal: 560ms\tremaining: 5.66s\n",
      "9:\tlearn: 0.4833605\ttotal: 619ms\tremaining: 5.57s\n",
      "10:\tlearn: 0.4709821\ttotal: 681ms\tremaining: 5.51s\n",
      "11:\tlearn: 0.4583521\ttotal: 741ms\tremaining: 5.43s\n",
      "12:\tlearn: 0.4479813\ttotal: 801ms\tremaining: 5.36s\n",
      "13:\tlearn: 0.4365533\ttotal: 858ms\tremaining: 5.27s\n",
      "14:\tlearn: 0.4281111\ttotal: 918ms\tremaining: 5.2s\n",
      "15:\tlearn: 0.4173625\ttotal: 981ms\tremaining: 5.15s\n",
      "16:\tlearn: 0.4079063\ttotal: 1.04s\tremaining: 5.09s\n",
      "17:\tlearn: 0.3991745\ttotal: 1.1s\tremaining: 5.03s\n",
      "18:\tlearn: 0.3907316\ttotal: 1.17s\tremaining: 4.98s\n",
      "19:\tlearn: 0.3843527\ttotal: 1.23s\tremaining: 4.91s\n",
      "20:\tlearn: 0.3779889\ttotal: 1.29s\tremaining: 4.85s\n",
      "21:\tlearn: 0.3717902\ttotal: 1.35s\tremaining: 4.79s\n",
      "22:\tlearn: 0.3656206\ttotal: 1.41s\tremaining: 4.73s\n",
      "23:\tlearn: 0.3600720\ttotal: 1.47s\tremaining: 4.67s\n",
      "24:\tlearn: 0.3558244\ttotal: 1.53s\tremaining: 4.59s\n",
      "25:\tlearn: 0.3521782\ttotal: 1.59s\tremaining: 4.52s\n",
      "26:\tlearn: 0.3478465\ttotal: 1.65s\tremaining: 4.46s\n",
      "27:\tlearn: 0.3438038\ttotal: 1.71s\tremaining: 4.39s\n",
      "28:\tlearn: 0.3389379\ttotal: 1.77s\tremaining: 4.33s\n",
      "29:\tlearn: 0.3361117\ttotal: 1.83s\tremaining: 4.27s\n",
      "30:\tlearn: 0.3324605\ttotal: 1.89s\tremaining: 4.2s\n",
      "31:\tlearn: 0.3280829\ttotal: 1.95s\tremaining: 4.14s\n",
      "32:\tlearn: 0.3241100\ttotal: 2.01s\tremaining: 4.08s\n",
      "33:\tlearn: 0.3207799\ttotal: 2.07s\tremaining: 4.02s\n",
      "34:\tlearn: 0.3179958\ttotal: 2.13s\tremaining: 3.96s\n",
      "35:\tlearn: 0.3153016\ttotal: 2.19s\tremaining: 3.9s\n",
      "36:\tlearn: 0.3115963\ttotal: 2.25s\tremaining: 3.83s\n",
      "37:\tlearn: 0.3078752\ttotal: 2.31s\tremaining: 3.77s\n",
      "38:\tlearn: 0.3055765\ttotal: 2.37s\tremaining: 3.71s\n",
      "39:\tlearn: 0.3033776\ttotal: 2.44s\tremaining: 3.66s\n",
      "40:\tlearn: 0.3006207\ttotal: 2.5s\tremaining: 3.6s\n",
      "41:\tlearn: 0.2978412\ttotal: 2.57s\tremaining: 3.55s\n",
      "42:\tlearn: 0.2960310\ttotal: 2.63s\tremaining: 3.48s\n",
      "43:\tlearn: 0.2928851\ttotal: 2.69s\tremaining: 3.42s\n",
      "44:\tlearn: 0.2907095\ttotal: 2.74s\tremaining: 3.35s\n",
      "45:\tlearn: 0.2892661\ttotal: 2.8s\tremaining: 3.29s\n",
      "46:\tlearn: 0.2871881\ttotal: 2.86s\tremaining: 3.23s\n",
      "47:\tlearn: 0.2858548\ttotal: 2.92s\tremaining: 3.17s\n",
      "48:\tlearn: 0.2840450\ttotal: 2.99s\tremaining: 3.11s\n",
      "49:\tlearn: 0.2826484\ttotal: 3.05s\tremaining: 3.05s\n",
      "50:\tlearn: 0.2809866\ttotal: 3.11s\tremaining: 2.99s\n",
      "51:\tlearn: 0.2783875\ttotal: 3.17s\tremaining: 2.93s\n",
      "52:\tlearn: 0.2762328\ttotal: 3.23s\tremaining: 2.87s\n",
      "53:\tlearn: 0.2741255\ttotal: 3.31s\tremaining: 2.82s\n",
      "54:\tlearn: 0.2722181\ttotal: 3.37s\tremaining: 2.76s\n",
      "55:\tlearn: 0.2713816\ttotal: 3.43s\tremaining: 2.69s\n",
      "56:\tlearn: 0.2700583\ttotal: 3.49s\tremaining: 2.63s\n",
      "57:\tlearn: 0.2684879\ttotal: 3.55s\tremaining: 2.57s\n",
      "58:\tlearn: 0.2663806\ttotal: 3.61s\tremaining: 2.51s\n",
      "59:\tlearn: 0.2652421\ttotal: 3.68s\tremaining: 2.46s\n",
      "60:\tlearn: 0.2633928\ttotal: 3.77s\tremaining: 2.41s\n",
      "61:\tlearn: 0.2615585\ttotal: 3.85s\tremaining: 2.36s\n",
      "62:\tlearn: 0.2594813\ttotal: 3.92s\tremaining: 2.3s\n",
      "63:\tlearn: 0.2574445\ttotal: 4s\tremaining: 2.25s\n",
      "64:\tlearn: 0.2565492\ttotal: 4.06s\tremaining: 2.19s\n",
      "65:\tlearn: 0.2547841\ttotal: 4.13s\tremaining: 2.13s\n",
      "66:\tlearn: 0.2533427\ttotal: 4.2s\tremaining: 2.07s\n",
      "67:\tlearn: 0.2522356\ttotal: 4.26s\tremaining: 2.01s\n",
      "68:\tlearn: 0.2510584\ttotal: 4.33s\tremaining: 1.94s\n",
      "69:\tlearn: 0.2493956\ttotal: 4.38s\tremaining: 1.88s\n",
      "70:\tlearn: 0.2479133\ttotal: 4.45s\tremaining: 1.82s\n",
      "71:\tlearn: 0.2466623\ttotal: 4.51s\tremaining: 1.75s\n",
      "72:\tlearn: 0.2454493\ttotal: 4.56s\tremaining: 1.69s\n",
      "73:\tlearn: 0.2440970\ttotal: 4.63s\tremaining: 1.63s\n",
      "74:\tlearn: 0.2433015\ttotal: 4.71s\tremaining: 1.57s\n",
      "75:\tlearn: 0.2415794\ttotal: 4.77s\tremaining: 1.5s\n",
      "76:\tlearn: 0.2404592\ttotal: 4.83s\tremaining: 1.44s\n",
      "77:\tlearn: 0.2392339\ttotal: 4.89s\tremaining: 1.38s\n",
      "78:\tlearn: 0.2380164\ttotal: 4.95s\tremaining: 1.31s\n",
      "79:\tlearn: 0.2366396\ttotal: 5.01s\tremaining: 1.25s\n",
      "80:\tlearn: 0.2354918\ttotal: 5.07s\tremaining: 1.19s\n",
      "81:\tlearn: 0.2347994\ttotal: 5.13s\tremaining: 1.13s\n",
      "82:\tlearn: 0.2334146\ttotal: 5.2s\tremaining: 1.06s\n",
      "83:\tlearn: 0.2323032\ttotal: 5.25s\tremaining: 1s\n",
      "84:\tlearn: 0.2308120\ttotal: 5.31s\tremaining: 937ms\n",
      "85:\tlearn: 0.2297748\ttotal: 5.37s\tremaining: 874ms\n",
      "86:\tlearn: 0.2287037\ttotal: 5.43s\tremaining: 811ms\n",
      "87:\tlearn: 0.2276650\ttotal: 5.49s\tremaining: 748ms\n",
      "88:\tlearn: 0.2265677\ttotal: 5.55s\tremaining: 686ms\n",
      "89:\tlearn: 0.2245975\ttotal: 5.61s\tremaining: 623ms\n",
      "90:\tlearn: 0.2234796\ttotal: 5.67s\tremaining: 561ms\n",
      "91:\tlearn: 0.2222435\ttotal: 5.74s\tremaining: 499ms\n",
      "92:\tlearn: 0.2209861\ttotal: 5.8s\tremaining: 436ms\n",
      "93:\tlearn: 0.2199550\ttotal: 5.85s\tremaining: 374ms\n",
      "94:\tlearn: 0.2193634\ttotal: 5.91s\tremaining: 311ms\n",
      "95:\tlearn: 0.2178826\ttotal: 5.97s\tremaining: 249ms\n",
      "96:\tlearn: 0.2164302\ttotal: 6.03s\tremaining: 187ms\n",
      "97:\tlearn: 0.2148383\ttotal: 6.09s\tremaining: 124ms\n",
      "98:\tlearn: 0.2130131\ttotal: 6.15s\tremaining: 62.1ms\n",
      "99:\tlearn: 0.2124767\ttotal: 6.21s\tremaining: 0us\n",
      "0:\tlearn: 0.6587631\ttotal: 70.2ms\tremaining: 6.95s\n",
      "1:\tlearn: 0.6328805\ttotal: 150ms\tremaining: 7.35s\n",
      "2:\tlearn: 0.6032654\ttotal: 225ms\tremaining: 7.29s\n",
      "3:\tlearn: 0.5789623\ttotal: 294ms\tremaining: 7.06s\n",
      "4:\tlearn: 0.5539546\ttotal: 359ms\tremaining: 6.82s\n",
      "5:\tlearn: 0.5328609\ttotal: 419ms\tremaining: 6.56s\n",
      "6:\tlearn: 0.5143994\ttotal: 477ms\tremaining: 6.34s\n",
      "7:\tlearn: 0.4999665\ttotal: 543ms\tremaining: 6.25s\n",
      "8:\tlearn: 0.4843018\ttotal: 609ms\tremaining: 6.15s\n",
      "9:\tlearn: 0.4680464\ttotal: 669ms\tremaining: 6.02s\n",
      "10:\tlearn: 0.4546468\ttotal: 727ms\tremaining: 5.88s\n",
      "11:\tlearn: 0.4425693\ttotal: 788ms\tremaining: 5.78s\n",
      "12:\tlearn: 0.4300129\ttotal: 849ms\tremaining: 5.68s\n",
      "13:\tlearn: 0.4176776\ttotal: 906ms\tremaining: 5.56s\n",
      "14:\tlearn: 0.4076149\ttotal: 971ms\tremaining: 5.5s\n",
      "15:\tlearn: 0.3987826\ttotal: 1.03s\tremaining: 5.41s\n",
      "16:\tlearn: 0.3904720\ttotal: 1.09s\tremaining: 5.31s\n",
      "17:\tlearn: 0.3808553\ttotal: 1.15s\tremaining: 5.23s\n",
      "18:\tlearn: 0.3734485\ttotal: 1.21s\tremaining: 5.14s\n",
      "19:\tlearn: 0.3647567\ttotal: 1.27s\tremaining: 5.07s\n",
      "20:\tlearn: 0.3580941\ttotal: 1.33s\tremaining: 5s\n",
      "21:\tlearn: 0.3531377\ttotal: 1.39s\tremaining: 4.91s\n",
      "22:\tlearn: 0.3478945\ttotal: 1.45s\tremaining: 4.84s\n",
      "23:\tlearn: 0.3428085\ttotal: 1.51s\tremaining: 4.77s\n",
      "24:\tlearn: 0.3384560\ttotal: 1.56s\tremaining: 4.7s\n",
      "25:\tlearn: 0.3325175\ttotal: 1.62s\tremaining: 4.62s\n",
      "26:\tlearn: 0.3292126\ttotal: 1.68s\tremaining: 4.55s\n",
      "27:\tlearn: 0.3269362\ttotal: 1.75s\tremaining: 4.5s\n",
      "28:\tlearn: 0.3211214\ttotal: 1.81s\tremaining: 4.43s\n",
      "29:\tlearn: 0.3189596\ttotal: 1.86s\tremaining: 4.35s\n",
      "30:\tlearn: 0.3141296\ttotal: 1.93s\tremaining: 4.29s\n",
      "31:\tlearn: 0.3104075\ttotal: 1.99s\tremaining: 4.23s\n",
      "32:\tlearn: 0.3071160\ttotal: 2.05s\tremaining: 4.17s\n",
      "33:\tlearn: 0.3043703\ttotal: 2.11s\tremaining: 4.09s\n",
      "34:\tlearn: 0.3007727\ttotal: 2.16s\tremaining: 4.02s\n",
      "35:\tlearn: 0.2973461\ttotal: 2.22s\tremaining: 3.95s\n",
      "36:\tlearn: 0.2933983\ttotal: 2.28s\tremaining: 3.89s\n",
      "37:\tlearn: 0.2898225\ttotal: 2.34s\tremaining: 3.82s\n",
      "38:\tlearn: 0.2869948\ttotal: 2.4s\tremaining: 3.76s\n",
      "39:\tlearn: 0.2851094\ttotal: 2.46s\tremaining: 3.7s\n",
      "40:\tlearn: 0.2832846\ttotal: 2.52s\tremaining: 3.63s\n",
      "41:\tlearn: 0.2806725\ttotal: 2.58s\tremaining: 3.56s\n",
      "42:\tlearn: 0.2790200\ttotal: 2.64s\tremaining: 3.5s\n",
      "43:\tlearn: 0.2761533\ttotal: 2.7s\tremaining: 3.43s\n",
      "44:\tlearn: 0.2736882\ttotal: 2.76s\tremaining: 3.37s\n",
      "45:\tlearn: 0.2716957\ttotal: 2.82s\tremaining: 3.31s\n",
      "46:\tlearn: 0.2683834\ttotal: 2.88s\tremaining: 3.25s\n",
      "47:\tlearn: 0.2666758\ttotal: 2.96s\tremaining: 3.2s\n",
      "48:\tlearn: 0.2648491\ttotal: 3.02s\tremaining: 3.15s\n",
      "49:\tlearn: 0.2630018\ttotal: 3.08s\tremaining: 3.08s\n",
      "50:\tlearn: 0.2614421\ttotal: 3.15s\tremaining: 3.02s\n",
      "51:\tlearn: 0.2595827\ttotal: 3.21s\tremaining: 2.96s\n",
      "52:\tlearn: 0.2576813\ttotal: 3.27s\tremaining: 2.9s\n",
      "53:\tlearn: 0.2563964\ttotal: 3.33s\tremaining: 2.84s\n",
      "54:\tlearn: 0.2546155\ttotal: 3.39s\tremaining: 2.77s\n",
      "55:\tlearn: 0.2529914\ttotal: 3.45s\tremaining: 2.71s\n",
      "56:\tlearn: 0.2518785\ttotal: 3.51s\tremaining: 2.65s\n",
      "57:\tlearn: 0.2488399\ttotal: 3.58s\tremaining: 2.59s\n",
      "58:\tlearn: 0.2468110\ttotal: 3.64s\tremaining: 2.53s\n",
      "59:\tlearn: 0.2454245\ttotal: 3.7s\tremaining: 2.47s\n",
      "60:\tlearn: 0.2432133\ttotal: 3.76s\tremaining: 2.4s\n",
      "61:\tlearn: 0.2416735\ttotal: 3.81s\tremaining: 2.34s\n",
      "62:\tlearn: 0.2399163\ttotal: 3.87s\tremaining: 2.27s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63:\tlearn: 0.2383747\ttotal: 3.96s\tremaining: 2.23s\n",
      "64:\tlearn: 0.2366799\ttotal: 4.03s\tremaining: 2.17s\n",
      "65:\tlearn: 0.2352225\ttotal: 4.1s\tremaining: 2.11s\n",
      "66:\tlearn: 0.2331595\ttotal: 4.17s\tremaining: 2.06s\n",
      "67:\tlearn: 0.2320415\ttotal: 4.25s\tremaining: 2s\n",
      "68:\tlearn: 0.2306371\ttotal: 4.32s\tremaining: 1.94s\n",
      "69:\tlearn: 0.2287495\ttotal: 4.4s\tremaining: 1.89s\n",
      "70:\tlearn: 0.2271178\ttotal: 4.48s\tremaining: 1.83s\n",
      "71:\tlearn: 0.2259501\ttotal: 4.55s\tremaining: 1.77s\n",
      "72:\tlearn: 0.2250713\ttotal: 4.64s\tremaining: 1.72s\n",
      "73:\tlearn: 0.2229224\ttotal: 4.75s\tremaining: 1.67s\n",
      "74:\tlearn: 0.2220490\ttotal: 4.84s\tremaining: 1.61s\n",
      "75:\tlearn: 0.2213716\ttotal: 4.91s\tremaining: 1.55s\n",
      "76:\tlearn: 0.2206287\ttotal: 4.98s\tremaining: 1.49s\n",
      "77:\tlearn: 0.2193944\ttotal: 5.06s\tremaining: 1.43s\n",
      "78:\tlearn: 0.2183734\ttotal: 5.13s\tremaining: 1.36s\n",
      "79:\tlearn: 0.2175523\ttotal: 5.2s\tremaining: 1.3s\n",
      "80:\tlearn: 0.2163171\ttotal: 5.27s\tremaining: 1.24s\n",
      "81:\tlearn: 0.2157887\ttotal: 5.33s\tremaining: 1.17s\n",
      "82:\tlearn: 0.2148938\ttotal: 5.41s\tremaining: 1.11s\n",
      "83:\tlearn: 0.2137743\ttotal: 5.47s\tremaining: 1.04s\n",
      "84:\tlearn: 0.2122411\ttotal: 5.54s\tremaining: 978ms\n",
      "85:\tlearn: 0.2112730\ttotal: 5.61s\tremaining: 914ms\n",
      "86:\tlearn: 0.2100234\ttotal: 5.67s\tremaining: 847ms\n",
      "87:\tlearn: 0.2091133\ttotal: 5.74s\tremaining: 783ms\n",
      "88:\tlearn: 0.2080134\ttotal: 5.82s\tremaining: 719ms\n",
      "89:\tlearn: 0.2067533\ttotal: 5.88s\tremaining: 653ms\n",
      "90:\tlearn: 0.2053998\ttotal: 5.95s\tremaining: 588ms\n",
      "91:\tlearn: 0.2039283\ttotal: 6s\tremaining: 522ms\n",
      "92:\tlearn: 0.2031389\ttotal: 6.07s\tremaining: 457ms\n",
      "93:\tlearn: 0.2024863\ttotal: 6.13s\tremaining: 391ms\n",
      "94:\tlearn: 0.2019544\ttotal: 6.19s\tremaining: 326ms\n",
      "95:\tlearn: 0.2008223\ttotal: 6.25s\tremaining: 261ms\n",
      "96:\tlearn: 0.1998118\ttotal: 6.32s\tremaining: 196ms\n",
      "97:\tlearn: 0.1986221\ttotal: 6.39s\tremaining: 131ms\n",
      "98:\tlearn: 0.1972925\ttotal: 6.46s\tremaining: 65.3ms\n",
      "99:\tlearn: 0.1962994\ttotal: 6.53s\tremaining: 0us\n",
      "0:\tlearn: 0.6570852\ttotal: 84.1ms\tremaining: 8.32s\n",
      "1:\tlearn: 0.6323219\ttotal: 152ms\tremaining: 7.46s\n",
      "2:\tlearn: 0.6040888\ttotal: 220ms\tremaining: 7.12s\n",
      "3:\tlearn: 0.5798489\ttotal: 286ms\tremaining: 6.86s\n",
      "4:\tlearn: 0.5546539\ttotal: 358ms\tremaining: 6.79s\n",
      "5:\tlearn: 0.5338423\ttotal: 428ms\tremaining: 6.7s\n",
      "6:\tlearn: 0.5136241\ttotal: 491ms\tremaining: 6.52s\n",
      "7:\tlearn: 0.4973061\ttotal: 563ms\tremaining: 6.47s\n",
      "8:\tlearn: 0.4817411\ttotal: 632ms\tremaining: 6.39s\n",
      "9:\tlearn: 0.4684801\ttotal: 698ms\tremaining: 6.28s\n",
      "10:\tlearn: 0.4550986\ttotal: 766ms\tremaining: 6.2s\n",
      "11:\tlearn: 0.4456622\ttotal: 849ms\tremaining: 6.22s\n",
      "12:\tlearn: 0.4373377\ttotal: 930ms\tremaining: 6.22s\n",
      "13:\tlearn: 0.4262186\ttotal: 1s\tremaining: 6.15s\n",
      "14:\tlearn: 0.4168744\ttotal: 1.07s\tremaining: 6.07s\n",
      "15:\tlearn: 0.4088108\ttotal: 1.15s\tremaining: 6.04s\n",
      "16:\tlearn: 0.4005834\ttotal: 1.22s\tremaining: 5.94s\n",
      "17:\tlearn: 0.3919048\ttotal: 1.29s\tremaining: 5.87s\n",
      "18:\tlearn: 0.3843310\ttotal: 1.36s\tremaining: 5.79s\n",
      "19:\tlearn: 0.3760609\ttotal: 1.43s\tremaining: 5.72s\n",
      "20:\tlearn: 0.3691079\ttotal: 1.5s\tremaining: 5.66s\n",
      "21:\tlearn: 0.3636339\ttotal: 1.57s\tremaining: 5.56s\n",
      "22:\tlearn: 0.3576378\ttotal: 1.63s\tremaining: 5.45s\n",
      "23:\tlearn: 0.3521486\ttotal: 1.7s\tremaining: 5.37s\n",
      "24:\tlearn: 0.3468234\ttotal: 1.76s\tremaining: 5.27s\n",
      "25:\tlearn: 0.3431434\ttotal: 1.82s\tremaining: 5.18s\n",
      "26:\tlearn: 0.3389373\ttotal: 1.89s\tremaining: 5.1s\n",
      "27:\tlearn: 0.3355604\ttotal: 1.95s\tremaining: 5.01s\n",
      "28:\tlearn: 0.3298503\ttotal: 2.01s\tremaining: 4.92s\n",
      "29:\tlearn: 0.3265305\ttotal: 2.08s\tremaining: 4.85s\n",
      "30:\tlearn: 0.3223405\ttotal: 2.14s\tremaining: 4.77s\n",
      "31:\tlearn: 0.3191807\ttotal: 2.21s\tremaining: 4.71s\n",
      "32:\tlearn: 0.3154578\ttotal: 2.27s\tremaining: 4.61s\n",
      "33:\tlearn: 0.3120797\ttotal: 2.33s\tremaining: 4.53s\n",
      "34:\tlearn: 0.3084231\ttotal: 2.39s\tremaining: 4.44s\n",
      "35:\tlearn: 0.3063291\ttotal: 2.45s\tremaining: 4.36s\n",
      "36:\tlearn: 0.3028653\ttotal: 2.51s\tremaining: 4.27s\n",
      "37:\tlearn: 0.2989153\ttotal: 2.57s\tremaining: 4.19s\n",
      "38:\tlearn: 0.2960377\ttotal: 2.63s\tremaining: 4.12s\n",
      "39:\tlearn: 0.2943087\ttotal: 2.69s\tremaining: 4.04s\n",
      "40:\tlearn: 0.2911025\ttotal: 2.76s\tremaining: 3.97s\n",
      "41:\tlearn: 0.2885978\ttotal: 2.82s\tremaining: 3.9s\n",
      "42:\tlearn: 0.2861252\ttotal: 2.89s\tremaining: 3.83s\n",
      "43:\tlearn: 0.2837482\ttotal: 2.95s\tremaining: 3.75s\n",
      "44:\tlearn: 0.2820092\ttotal: 3.02s\tremaining: 3.69s\n",
      "45:\tlearn: 0.2802172\ttotal: 3.1s\tremaining: 3.63s\n",
      "46:\tlearn: 0.2782764\ttotal: 3.16s\tremaining: 3.56s\n",
      "47:\tlearn: 0.2770444\ttotal: 3.22s\tremaining: 3.49s\n",
      "48:\tlearn: 0.2748059\ttotal: 3.29s\tremaining: 3.42s\n",
      "49:\tlearn: 0.2721925\ttotal: 3.35s\tremaining: 3.35s\n",
      "50:\tlearn: 0.2706511\ttotal: 3.41s\tremaining: 3.28s\n",
      "51:\tlearn: 0.2682420\ttotal: 3.47s\tremaining: 3.21s\n",
      "52:\tlearn: 0.2664917\ttotal: 3.54s\tremaining: 3.14s\n",
      "53:\tlearn: 0.2646548\ttotal: 3.61s\tremaining: 3.07s\n",
      "54:\tlearn: 0.2630178\ttotal: 3.67s\tremaining: 3.01s\n",
      "55:\tlearn: 0.2612090\ttotal: 3.74s\tremaining: 2.94s\n",
      "56:\tlearn: 0.2599502\ttotal: 3.8s\tremaining: 2.87s\n",
      "57:\tlearn: 0.2582745\ttotal: 3.86s\tremaining: 2.8s\n",
      "58:\tlearn: 0.2566598\ttotal: 3.92s\tremaining: 2.73s\n",
      "59:\tlearn: 0.2548200\ttotal: 3.98s\tremaining: 2.65s\n",
      "60:\tlearn: 0.2532666\ttotal: 4.04s\tremaining: 2.58s\n",
      "61:\tlearn: 0.2515675\ttotal: 4.11s\tremaining: 2.52s\n",
      "62:\tlearn: 0.2500283\ttotal: 4.18s\tremaining: 2.45s\n",
      "63:\tlearn: 0.2483732\ttotal: 4.24s\tremaining: 2.38s\n",
      "64:\tlearn: 0.2462510\ttotal: 4.3s\tremaining: 2.32s\n",
      "65:\tlearn: 0.2442655\ttotal: 4.37s\tremaining: 2.25s\n",
      "66:\tlearn: 0.2424423\ttotal: 4.42s\tremaining: 2.18s\n",
      "67:\tlearn: 0.2418765\ttotal: 4.49s\tremaining: 2.11s\n",
      "68:\tlearn: 0.2402442\ttotal: 4.54s\tremaining: 2.04s\n",
      "69:\tlearn: 0.2386707\ttotal: 4.6s\tremaining: 1.97s\n",
      "70:\tlearn: 0.2375574\ttotal: 4.67s\tremaining: 1.91s\n",
      "71:\tlearn: 0.2362955\ttotal: 4.72s\tremaining: 1.84s\n",
      "72:\tlearn: 0.2352914\ttotal: 4.78s\tremaining: 1.77s\n",
      "73:\tlearn: 0.2342209\ttotal: 4.84s\tremaining: 1.7s\n",
      "74:\tlearn: 0.2326746\ttotal: 4.91s\tremaining: 1.64s\n",
      "75:\tlearn: 0.2315639\ttotal: 4.97s\tremaining: 1.57s\n",
      "76:\tlearn: 0.2306798\ttotal: 5.03s\tremaining: 1.5s\n",
      "77:\tlearn: 0.2289332\ttotal: 5.1s\tremaining: 1.44s\n",
      "78:\tlearn: 0.2276010\ttotal: 5.16s\tremaining: 1.37s\n",
      "79:\tlearn: 0.2267379\ttotal: 5.22s\tremaining: 1.3s\n",
      "80:\tlearn: 0.2249208\ttotal: 5.28s\tremaining: 1.24s\n",
      "81:\tlearn: 0.2241389\ttotal: 5.34s\tremaining: 1.17s\n",
      "82:\tlearn: 0.2226398\ttotal: 5.4s\tremaining: 1.11s\n",
      "83:\tlearn: 0.2212394\ttotal: 5.46s\tremaining: 1.04s\n",
      "84:\tlearn: 0.2198346\ttotal: 5.52s\tremaining: 974ms\n",
      "85:\tlearn: 0.2187650\ttotal: 5.58s\tremaining: 909ms\n",
      "86:\tlearn: 0.2179110\ttotal: 5.65s\tremaining: 844ms\n",
      "87:\tlearn: 0.2162459\ttotal: 5.72s\tremaining: 781ms\n",
      "88:\tlearn: 0.2152037\ttotal: 5.79s\tremaining: 716ms\n",
      "89:\tlearn: 0.2141340\ttotal: 5.85s\tremaining: 650ms\n",
      "90:\tlearn: 0.2132750\ttotal: 5.91s\tremaining: 585ms\n",
      "91:\tlearn: 0.2117306\ttotal: 5.97s\tremaining: 519ms\n",
      "92:\tlearn: 0.2104254\ttotal: 6.03s\tremaining: 454ms\n",
      "93:\tlearn: 0.2093839\ttotal: 6.09s\tremaining: 389ms\n",
      "94:\tlearn: 0.2077572\ttotal: 6.16s\tremaining: 324ms\n",
      "95:\tlearn: 0.2069785\ttotal: 6.22s\tremaining: 259ms\n",
      "96:\tlearn: 0.2061129\ttotal: 6.29s\tremaining: 195ms\n",
      "97:\tlearn: 0.2054761\ttotal: 6.35s\tremaining: 130ms\n",
      "98:\tlearn: 0.2049647\ttotal: 6.42s\tremaining: 64.8ms\n",
      "99:\tlearn: 0.2041581\ttotal: 6.48s\tremaining: 0us\n",
      "0:\tlearn: 0.6590791\ttotal: 77.4ms\tremaining: 7.66s\n",
      "1:\tlearn: 0.6354799\ttotal: 147ms\tremaining: 7.19s\n",
      "2:\tlearn: 0.6061122\ttotal: 208ms\tremaining: 6.73s\n",
      "3:\tlearn: 0.5825122\ttotal: 265ms\tremaining: 6.36s\n",
      "4:\tlearn: 0.5583256\ttotal: 329ms\tremaining: 6.25s\n",
      "5:\tlearn: 0.5368308\ttotal: 391ms\tremaining: 6.13s\n",
      "6:\tlearn: 0.5197746\ttotal: 451ms\tremaining: 5.99s\n",
      "7:\tlearn: 0.5019558\ttotal: 513ms\tremaining: 5.9s\n",
      "8:\tlearn: 0.4889737\ttotal: 581ms\tremaining: 5.87s\n",
      "9:\tlearn: 0.4744986\ttotal: 642ms\tremaining: 5.78s\n",
      "10:\tlearn: 0.4610515\ttotal: 706ms\tremaining: 5.71s\n",
      "11:\tlearn: 0.4508209\ttotal: 768ms\tremaining: 5.63s\n",
      "12:\tlearn: 0.4393846\ttotal: 833ms\tremaining: 5.57s\n",
      "13:\tlearn: 0.4278870\ttotal: 899ms\tremaining: 5.52s\n",
      "14:\tlearn: 0.4184398\ttotal: 965ms\tremaining: 5.47s\n",
      "15:\tlearn: 0.4105298\ttotal: 1.03s\tremaining: 5.4s\n",
      "16:\tlearn: 0.4034400\ttotal: 1.09s\tremaining: 5.34s\n",
      "17:\tlearn: 0.3960907\ttotal: 1.15s\tremaining: 5.25s\n",
      "18:\tlearn: 0.3876144\ttotal: 1.21s\tremaining: 5.17s\n",
      "19:\tlearn: 0.3789381\ttotal: 1.27s\tremaining: 5.1s\n",
      "20:\tlearn: 0.3717255\ttotal: 1.34s\tremaining: 5.06s\n",
      "21:\tlearn: 0.3647702\ttotal: 1.41s\tremaining: 4.99s\n",
      "22:\tlearn: 0.3611083\ttotal: 1.47s\tremaining: 4.91s\n",
      "23:\tlearn: 0.3551166\ttotal: 1.52s\tremaining: 4.83s\n",
      "24:\tlearn: 0.3510729\ttotal: 1.58s\tremaining: 4.75s\n",
      "25:\tlearn: 0.3453593\ttotal: 1.64s\tremaining: 4.68s\n",
      "26:\tlearn: 0.3426552\ttotal: 1.71s\tremaining: 4.62s\n",
      "27:\tlearn: 0.3394267\ttotal: 1.77s\tremaining: 4.55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28:\tlearn: 0.3334037\ttotal: 1.83s\tremaining: 4.48s\n",
      "29:\tlearn: 0.3302369\ttotal: 1.89s\tremaining: 4.41s\n",
      "30:\tlearn: 0.3262998\ttotal: 1.95s\tremaining: 4.34s\n",
      "31:\tlearn: 0.3228219\ttotal: 2.01s\tremaining: 4.27s\n",
      "32:\tlearn: 0.3196775\ttotal: 2.08s\tremaining: 4.23s\n",
      "33:\tlearn: 0.3168290\ttotal: 2.14s\tremaining: 4.16s\n",
      "34:\tlearn: 0.3126414\ttotal: 2.2s\tremaining: 4.09s\n",
      "35:\tlearn: 0.3101018\ttotal: 2.26s\tremaining: 4.02s\n",
      "36:\tlearn: 0.3071472\ttotal: 2.33s\tremaining: 3.96s\n",
      "37:\tlearn: 0.3033529\ttotal: 2.39s\tremaining: 3.9s\n",
      "38:\tlearn: 0.2997535\ttotal: 2.45s\tremaining: 3.83s\n",
      "39:\tlearn: 0.2979998\ttotal: 2.5s\tremaining: 3.76s\n",
      "40:\tlearn: 0.2962753\ttotal: 2.56s\tremaining: 3.69s\n",
      "41:\tlearn: 0.2935294\ttotal: 2.62s\tremaining: 3.62s\n",
      "42:\tlearn: 0.2913436\ttotal: 2.68s\tremaining: 3.55s\n",
      "43:\tlearn: 0.2882137\ttotal: 2.74s\tremaining: 3.49s\n",
      "44:\tlearn: 0.2860258\ttotal: 2.8s\tremaining: 3.42s\n",
      "45:\tlearn: 0.2836975\ttotal: 2.86s\tremaining: 3.36s\n",
      "46:\tlearn: 0.2819123\ttotal: 2.92s\tremaining: 3.29s\n",
      "47:\tlearn: 0.2802962\ttotal: 2.98s\tremaining: 3.23s\n",
      "48:\tlearn: 0.2774617\ttotal: 3.04s\tremaining: 3.17s\n",
      "49:\tlearn: 0.2754419\ttotal: 3.11s\tremaining: 3.11s\n",
      "50:\tlearn: 0.2739001\ttotal: 3.17s\tremaining: 3.04s\n",
      "51:\tlearn: 0.2719283\ttotal: 3.22s\tremaining: 2.98s\n",
      "52:\tlearn: 0.2701538\ttotal: 3.28s\tremaining: 2.91s\n",
      "53:\tlearn: 0.2685414\ttotal: 3.35s\tremaining: 2.85s\n",
      "54:\tlearn: 0.2658507\ttotal: 3.4s\tremaining: 2.79s\n",
      "55:\tlearn: 0.2634405\ttotal: 3.46s\tremaining: 2.72s\n",
      "56:\tlearn: 0.2624837\ttotal: 3.53s\tremaining: 2.66s\n",
      "57:\tlearn: 0.2603108\ttotal: 3.58s\tremaining: 2.6s\n",
      "58:\tlearn: 0.2589809\ttotal: 3.64s\tremaining: 2.53s\n",
      "59:\tlearn: 0.2575771\ttotal: 3.7s\tremaining: 2.47s\n",
      "60:\tlearn: 0.2562463\ttotal: 3.77s\tremaining: 2.41s\n",
      "61:\tlearn: 0.2550877\ttotal: 3.83s\tremaining: 2.35s\n",
      "62:\tlearn: 0.2538510\ttotal: 3.89s\tremaining: 2.29s\n",
      "63:\tlearn: 0.2525568\ttotal: 3.96s\tremaining: 2.23s\n",
      "64:\tlearn: 0.2514700\ttotal: 4.03s\tremaining: 2.17s\n",
      "65:\tlearn: 0.2494654\ttotal: 4.09s\tremaining: 2.11s\n",
      "66:\tlearn: 0.2479144\ttotal: 4.15s\tremaining: 2.04s\n",
      "67:\tlearn: 0.2469608\ttotal: 4.21s\tremaining: 1.98s\n",
      "68:\tlearn: 0.2455417\ttotal: 4.28s\tremaining: 1.92s\n",
      "69:\tlearn: 0.2443353\ttotal: 4.33s\tremaining: 1.86s\n",
      "70:\tlearn: 0.2432227\ttotal: 4.39s\tremaining: 1.79s\n",
      "71:\tlearn: 0.2417518\ttotal: 4.45s\tremaining: 1.73s\n",
      "72:\tlearn: 0.2402587\ttotal: 4.51s\tremaining: 1.67s\n",
      "73:\tlearn: 0.2393126\ttotal: 4.57s\tremaining: 1.6s\n",
      "74:\tlearn: 0.2377120\ttotal: 4.62s\tremaining: 1.54s\n",
      "75:\tlearn: 0.2369019\ttotal: 4.69s\tremaining: 1.48s\n",
      "76:\tlearn: 0.2357246\ttotal: 4.75s\tremaining: 1.42s\n",
      "77:\tlearn: 0.2347963\ttotal: 4.8s\tremaining: 1.35s\n",
      "78:\tlearn: 0.2340700\ttotal: 4.87s\tremaining: 1.29s\n",
      "79:\tlearn: 0.2330808\ttotal: 4.93s\tremaining: 1.23s\n",
      "80:\tlearn: 0.2318769\ttotal: 4.99s\tremaining: 1.17s\n",
      "81:\tlearn: 0.2307377\ttotal: 5.05s\tremaining: 1.11s\n",
      "82:\tlearn: 0.2294090\ttotal: 5.11s\tremaining: 1.05s\n",
      "83:\tlearn: 0.2282971\ttotal: 5.17s\tremaining: 985ms\n",
      "84:\tlearn: 0.2267912\ttotal: 5.23s\tremaining: 923ms\n",
      "85:\tlearn: 0.2255245\ttotal: 5.29s\tremaining: 861ms\n",
      "86:\tlearn: 0.2245381\ttotal: 5.35s\tremaining: 799ms\n",
      "87:\tlearn: 0.2227696\ttotal: 5.41s\tremaining: 738ms\n",
      "88:\tlearn: 0.2217958\ttotal: 5.47s\tremaining: 676ms\n",
      "89:\tlearn: 0.2202404\ttotal: 5.53s\tremaining: 614ms\n",
      "90:\tlearn: 0.2186921\ttotal: 5.59s\tremaining: 553ms\n",
      "91:\tlearn: 0.2178358\ttotal: 5.65s\tremaining: 491ms\n",
      "92:\tlearn: 0.2166083\ttotal: 5.7s\tremaining: 429ms\n",
      "93:\tlearn: 0.2156478\ttotal: 5.76s\tremaining: 368ms\n",
      "94:\tlearn: 0.2145779\ttotal: 5.82s\tremaining: 306ms\n",
      "95:\tlearn: 0.2132316\ttotal: 5.88s\tremaining: 245ms\n",
      "96:\tlearn: 0.2123499\ttotal: 5.94s\tremaining: 184ms\n",
      "97:\tlearn: 0.2108571\ttotal: 6s\tremaining: 123ms\n",
      "98:\tlearn: 0.2098382\ttotal: 6.06s\tremaining: 61.2ms\n",
      "99:\tlearn: 0.2089062\ttotal: 6.13s\tremaining: 0us\n",
      "0:\tlearn: 0.6596673\ttotal: 67.9ms\tremaining: 6.72s\n",
      "1:\tlearn: 0.6334991\ttotal: 130ms\tremaining: 6.39s\n",
      "2:\tlearn: 0.6067959\ttotal: 193ms\tremaining: 6.23s\n",
      "3:\tlearn: 0.5842094\ttotal: 252ms\tremaining: 6.04s\n",
      "4:\tlearn: 0.5611662\ttotal: 313ms\tremaining: 5.95s\n",
      "5:\tlearn: 0.5417554\ttotal: 382ms\tremaining: 5.99s\n",
      "6:\tlearn: 0.5257208\ttotal: 467ms\tremaining: 6.2s\n",
      "7:\tlearn: 0.5109656\ttotal: 544ms\tremaining: 6.25s\n",
      "8:\tlearn: 0.4950242\ttotal: 624ms\tremaining: 6.31s\n",
      "9:\tlearn: 0.4798258\ttotal: 694ms\tremaining: 6.24s\n",
      "10:\tlearn: 0.4670071\ttotal: 761ms\tremaining: 6.16s\n",
      "11:\tlearn: 0.4570000\ttotal: 822ms\tremaining: 6.03s\n",
      "12:\tlearn: 0.4467485\ttotal: 892ms\tremaining: 5.97s\n",
      "13:\tlearn: 0.4345887\ttotal: 952ms\tremaining: 5.84s\n",
      "14:\tlearn: 0.4234507\ttotal: 1.01s\tremaining: 5.75s\n",
      "15:\tlearn: 0.4155224\ttotal: 1.08s\tremaining: 5.69s\n",
      "16:\tlearn: 0.4080055\ttotal: 1.15s\tremaining: 5.62s\n",
      "17:\tlearn: 0.3999147\ttotal: 1.22s\tremaining: 5.54s\n",
      "18:\tlearn: 0.3933561\ttotal: 1.28s\tremaining: 5.47s\n",
      "19:\tlearn: 0.3848942\ttotal: 1.34s\tremaining: 5.38s\n",
      "20:\tlearn: 0.3784211\ttotal: 1.42s\tremaining: 5.33s\n",
      "21:\tlearn: 0.3728466\ttotal: 1.49s\tremaining: 5.28s\n",
      "22:\tlearn: 0.3673622\ttotal: 1.56s\tremaining: 5.23s\n",
      "23:\tlearn: 0.3627247\ttotal: 1.63s\tremaining: 5.15s\n",
      "24:\tlearn: 0.3583059\ttotal: 1.7s\tremaining: 5.1s\n",
      "25:\tlearn: 0.3523969\ttotal: 1.77s\tremaining: 5.05s\n",
      "26:\tlearn: 0.3476433\ttotal: 1.85s\tremaining: 4.99s\n",
      "27:\tlearn: 0.3435029\ttotal: 1.92s\tremaining: 4.93s\n",
      "28:\tlearn: 0.3396544\ttotal: 1.98s\tremaining: 4.85s\n",
      "29:\tlearn: 0.3367781\ttotal: 2.04s\tremaining: 4.77s\n",
      "30:\tlearn: 0.3342433\ttotal: 2.11s\tremaining: 4.69s\n",
      "31:\tlearn: 0.3294611\ttotal: 2.17s\tremaining: 4.62s\n",
      "32:\tlearn: 0.3268883\ttotal: 2.24s\tremaining: 4.55s\n",
      "33:\tlearn: 0.3235952\ttotal: 2.31s\tremaining: 4.48s\n",
      "34:\tlearn: 0.3201081\ttotal: 2.37s\tremaining: 4.4s\n",
      "35:\tlearn: 0.3174008\ttotal: 2.43s\tremaining: 4.32s\n",
      "36:\tlearn: 0.3144433\ttotal: 2.49s\tremaining: 4.24s\n",
      "37:\tlearn: 0.3110325\ttotal: 2.55s\tremaining: 4.16s\n",
      "38:\tlearn: 0.3090854\ttotal: 2.6s\tremaining: 4.08s\n",
      "39:\tlearn: 0.3057052\ttotal: 2.67s\tremaining: 4.01s\n",
      "40:\tlearn: 0.3031059\ttotal: 2.76s\tremaining: 3.97s\n",
      "41:\tlearn: 0.3000213\ttotal: 2.83s\tremaining: 3.91s\n",
      "42:\tlearn: 0.2981074\ttotal: 2.89s\tremaining: 3.83s\n",
      "43:\tlearn: 0.2950468\ttotal: 2.96s\tremaining: 3.77s\n",
      "44:\tlearn: 0.2928740\ttotal: 3.03s\tremaining: 3.7s\n",
      "45:\tlearn: 0.2910297\ttotal: 3.09s\tremaining: 3.63s\n",
      "46:\tlearn: 0.2885417\ttotal: 3.15s\tremaining: 3.56s\n",
      "47:\tlearn: 0.2872753\ttotal: 3.22s\tremaining: 3.49s\n",
      "48:\tlearn: 0.2852469\ttotal: 3.29s\tremaining: 3.42s\n",
      "49:\tlearn: 0.2826160\ttotal: 3.35s\tremaining: 3.35s\n",
      "50:\tlearn: 0.2807145\ttotal: 3.41s\tremaining: 3.28s\n",
      "51:\tlearn: 0.2788405\ttotal: 3.5s\tremaining: 3.23s\n",
      "52:\tlearn: 0.2772699\ttotal: 3.57s\tremaining: 3.17s\n",
      "53:\tlearn: 0.2755647\ttotal: 3.64s\tremaining: 3.1s\n",
      "54:\tlearn: 0.2733886\ttotal: 3.71s\tremaining: 3.03s\n",
      "55:\tlearn: 0.2715888\ttotal: 3.78s\tremaining: 2.97s\n",
      "56:\tlearn: 0.2696901\ttotal: 3.85s\tremaining: 2.9s\n",
      "57:\tlearn: 0.2682433\ttotal: 3.91s\tremaining: 2.83s\n",
      "58:\tlearn: 0.2666873\ttotal: 3.98s\tremaining: 2.77s\n",
      "59:\tlearn: 0.2652906\ttotal: 4.05s\tremaining: 2.7s\n",
      "60:\tlearn: 0.2639622\ttotal: 4.12s\tremaining: 2.63s\n",
      "61:\tlearn: 0.2629201\ttotal: 4.19s\tremaining: 2.57s\n",
      "62:\tlearn: 0.2610690\ttotal: 4.27s\tremaining: 2.51s\n",
      "63:\tlearn: 0.2593729\ttotal: 4.34s\tremaining: 2.44s\n",
      "64:\tlearn: 0.2583483\ttotal: 4.42s\tremaining: 2.38s\n",
      "65:\tlearn: 0.2568905\ttotal: 4.51s\tremaining: 2.32s\n",
      "66:\tlearn: 0.2556907\ttotal: 4.59s\tremaining: 2.26s\n",
      "67:\tlearn: 0.2549145\ttotal: 4.66s\tremaining: 2.19s\n",
      "68:\tlearn: 0.2536683\ttotal: 4.76s\tremaining: 2.14s\n",
      "69:\tlearn: 0.2523025\ttotal: 4.83s\tremaining: 2.07s\n",
      "70:\tlearn: 0.2505020\ttotal: 4.9s\tremaining: 2s\n",
      "71:\tlearn: 0.2490766\ttotal: 4.96s\tremaining: 1.93s\n",
      "72:\tlearn: 0.2483065\ttotal: 5.03s\tremaining: 1.86s\n",
      "73:\tlearn: 0.2468298\ttotal: 5.1s\tremaining: 1.79s\n",
      "74:\tlearn: 0.2456181\ttotal: 5.17s\tremaining: 1.72s\n",
      "75:\tlearn: 0.2443715\ttotal: 5.24s\tremaining: 1.65s\n",
      "76:\tlearn: 0.2434645\ttotal: 5.3s\tremaining: 1.58s\n",
      "77:\tlearn: 0.2421296\ttotal: 5.37s\tremaining: 1.51s\n",
      "78:\tlearn: 0.2406393\ttotal: 5.44s\tremaining: 1.45s\n",
      "79:\tlearn: 0.2397431\ttotal: 5.5s\tremaining: 1.38s\n",
      "80:\tlearn: 0.2384432\ttotal: 5.56s\tremaining: 1.3s\n",
      "81:\tlearn: 0.2369794\ttotal: 5.62s\tremaining: 1.23s\n",
      "82:\tlearn: 0.2355340\ttotal: 5.68s\tremaining: 1.16s\n",
      "83:\tlearn: 0.2340007\ttotal: 5.74s\tremaining: 1.09s\n",
      "84:\tlearn: 0.2327440\ttotal: 5.8s\tremaining: 1.02s\n",
      "85:\tlearn: 0.2316819\ttotal: 5.86s\tremaining: 954ms\n",
      "86:\tlearn: 0.2297382\ttotal: 5.93s\tremaining: 886ms\n",
      "87:\tlearn: 0.2287045\ttotal: 5.99s\tremaining: 816ms\n",
      "88:\tlearn: 0.2274924\ttotal: 6.05s\tremaining: 748ms\n",
      "89:\tlearn: 0.2259341\ttotal: 6.12s\tremaining: 679ms\n",
      "90:\tlearn: 0.2251676\ttotal: 6.17s\tremaining: 611ms\n",
      "91:\tlearn: 0.2242145\ttotal: 6.23s\tremaining: 542ms\n",
      "92:\tlearn: 0.2229372\ttotal: 6.29s\tremaining: 474ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93:\tlearn: 0.2222927\ttotal: 6.35s\tremaining: 405ms\n",
      "94:\tlearn: 0.2214478\ttotal: 6.41s\tremaining: 338ms\n",
      "95:\tlearn: 0.2203388\ttotal: 6.47s\tremaining: 270ms\n",
      "96:\tlearn: 0.2188129\ttotal: 6.53s\tremaining: 202ms\n",
      "97:\tlearn: 0.2171228\ttotal: 6.59s\tremaining: 135ms\n",
      "98:\tlearn: 0.2162284\ttotal: 6.66s\tremaining: 67.3ms\n",
      "99:\tlearn: 0.2146658\ttotal: 6.71s\tremaining: 0us\n",
      "SVC\n",
      "NeuralNetwork\n",
      "NaiveBayes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 169, in fit\n",
      "    X, y = self._validate_data(X, y, dtype=np.float64,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 169, in fit\n",
      "    X, y = self._validate_data(X, y, dtype=np.float64,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 169, in fit\n",
      "    X, y = self._validate_data(X, y, dtype=np.float64,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 169, in fit\n",
      "    X, y = self._validate_data(X, y, dtype=np.float64,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 169, in fit\n",
      "    X, y = self._validate_data(X, y, dtype=np.float64,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 364, in _fit\n",
      "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 968, in _validate_input\n",
      "    X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc'],\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 364, in _fit\n",
      "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 968, in _validate_input\n",
      "    X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc'],\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 364, in _fit\n",
      "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 968, in _validate_input\n",
      "    X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc'],\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 364, in _fit\n",
      "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 968, in _validate_input\n",
      "    X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc'],\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 364, in _fit\n",
      "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 968, in _validate_input\n",
      "    X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc'],\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 814, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"C:\\Users\\mehdi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.957466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.940015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeuralNetwork</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaiveBayes</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Score\n",
       "XGBoost        0.957466\n",
       "CatBoost       0.940015\n",
       "SVC                 NaN\n",
       "NeuralNetwork       NaN\n",
       "NaiveBayes          NaN"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = [\n",
    "    xgb.XGBClassifier(),\n",
    "    CBC,\n",
    "    SVC_classifier,\n",
    "    MLP_classifier,\n",
    "    Gaussian_NB,\n",
    "]\n",
    "clf_names = [\n",
    "    \"XGBoost\",\n",
    "    \"CatBoost\",\n",
    "    \"SVC\",\n",
    "    \"NeuralNetwork\",\n",
    "    \"NaiveBayes\",\n",
    "#     \"Gaussian\"\n",
    "]\n",
    "\n",
    "scores = np.zeros(len(clfs))\n",
    "\n",
    "\n",
    "\n",
    "for i in np.arange(len(clfs)):\n",
    "    print(clf_names[i])\n",
    "    scores[i] = np.mean(cross_validate(clfs[i], X, Y, scoring=\"roc_auc\", cv=5, return_train_score=False)[\"test_score\"])\n",
    "    \n",
    "pd.DataFrame(scores, index=clf_names, columns=[\"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5958481d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.957466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.940015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeuralNetwork</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaiveBayes</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Score\n",
       "XGBoost        0.957466\n",
       "CatBoost       0.940015\n",
       "SVC                 NaN\n",
       "NeuralNetwork       NaN\n",
       "NaiveBayes          NaN"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores, index=clf_names, columns=[\"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95dcc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb.XGBClassifier(),open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da5e9e5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[5. 5. 5. 5. 5.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-fb101a186637>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[1;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\",\n\u001b[0m\u001b[0;32m    403\u001b[0m                                     reset=False)\n\u001b[0;32m    404\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    638\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[5. 5. 5. 5. 5.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "prediction = model.predict([.5,5.,5.,5.,5.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d6d0a",
   "metadata": {},
   "source": [
    "## Even without a grid search, the XGBoost classifier has the best accuracy. We'll use it on the API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
